{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc77a4c1-352e-48cf-b9ff-faa9a493d5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting enhanced bot detection pipeline...\n",
      "Preprocessing data...\n",
      "Filtered to 126091 users with 2 or more posts\n",
      "created_at column type before conversion: datetime64[ns, UTC]\n",
      "Sample values before conversion: ['2023-01-01T11:38:00.000000000' '2023-01-05T06:36:56.000000000']\n",
      "First attempt conversion failures: 0\n",
      "Final datetime conversion failures: 0 (0.00%)\n",
      "Found 126091 unique authors, 126091 with 2+ posts\n",
      "Engineering features...\n",
      "Calculating content similarity...\n",
      "Analyzing behavioral patterns...\n",
      "Building interaction network and extracting graph features...\n",
      "Analyzing temporal behavioral patterns...\n",
      "Detecting coordinated behavior patterns...\n",
      "Removing 4 highly correlated features: ['std_time_between_posts', 'shape_entropy', 'avg_content_similarity', 'out_degree']\n",
      "Using 44 features for model training\n",
      "Detecting anomalies...\n",
      "Training ensemble classifier...\n",
      "Training with 86 trees, max_depth=8\n",
      "Training batch 1/9\n",
      "Training batch 2/9\n",
      "Training batch 3/9\n",
      "Training batch 4/9\n",
      "Training batch 5/9\n",
      "Training batch 6/9\n",
      "Training batch 7/9\n",
      "Training batch 8/9\n",
      "Training batch 9/9\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.96     40353\n",
      "           1       0.69      0.82      0.75      5253\n",
      "\n",
      "    accuracy                           0.94     45606\n",
      "   macro avg       0.83      0.88      0.86     45606\n",
      "weighted avg       0.94      0.94      0.94     45606\n",
      "\n",
      "[[38422  1931]\n",
      " [  964  4289]]\n",
      "Optimal threshold: 0.5683442581497714\n",
      "Using optimal threshold: 0.5771092125223045\n",
      "Aggregating results by author...\n",
      "Found 126091 users with 2+ posts\n",
      "Final bot detection results: 7287 bots out of 126091 total accounts\n",
      "Generating visualizations...\n",
      "Validating and saving results...\n",
      "Results validated and saved to bot_detection_results.csv\n",
      "Detected 7287 bot accounts out of 126091 total accounts\n",
      "Enhanced bot detection complete. Results saved to bot_detection_results.csv\n",
      "Detected 7287 bot accounts out of 126091 total accounts\n"
     ]
    }
   ],
   "source": [
    "# Code 1 \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import re\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.ensemble import IsolationForest, ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "import gc\n",
    "import warnings\n",
    "from scipy.stats import entropy\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Memory optimization - use efficient data types\n",
    "dtypes = {\n",
    "    'reply_count': 'int16',\n",
    "    'repost_count': 'int16',\n",
    "    'like_count': 'int16',\n",
    "    'happy': 'int8',\n",
    "    'sad': 'int8',\n",
    "    'neutral': 'int8'\n",
    "}\n",
    "\n",
    "def process_in_chunks(file_path, chunk_size=100000):\n",
    "    \"\"\"Process large datasets in chunks to avoid memory issues\"\"\"\n",
    "    chunks = []\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunk_size, dtype=dtypes):\n",
    "        chunks.append(chunk)\n",
    "    return pd.concat(chunks)\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"Preprocess the data and handle missing values\"\"\"\n",
    "    print(\"Preprocessing data...\")\n",
    "    \n",
    "    # First, ensure we filter down to only users with 2+ posts\n",
    "    # Count posts per user\n",
    "    post_counts = df['author_handle'].value_counts()\n",
    "    users_with_multiple_posts = post_counts[post_counts >= 2].index\n",
    "    \n",
    "    # Filter the dataframe to include only these users\n",
    "    df = df[df['author_handle'].isin(users_with_multiple_posts)].reset_index(drop=True)\n",
    "    print(f\"Filtered to {len(users_with_multiple_posts)} users with 2 or more posts\")\n",
    "    \n",
    "    # Fill missing values\n",
    "    for col in ['reply_count', 'repost_count', 'like_count', 'happy', 'sad']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].fillna(0)\n",
    "    \n",
    "    # Add neutral sentiment column if not present\n",
    "    if 'neutral' not in df.columns:\n",
    "        df['neutral'] = ((df['happy'] == 0) & (df['sad'] == 0)).astype(int)\n",
    "    \n",
    "    # Convert timestamp to datetime with improved error handling\n",
    "    if 'created_at' in df.columns:\n",
    "        print(f\"created_at column type before conversion: {df['created_at'].dtype}\")\n",
    "        print(f\"Sample values before conversion: {df['created_at'].head(2).values}\")\n",
    "        \n",
    "        # Create a copy of original timestamps before conversion attempts\n",
    "        df['created_at_original'] = df['created_at']\n",
    "        \n",
    "        # Try with a more flexible parser first with different formats\n",
    "        try:\n",
    "            # Try with coerce to handle errors gracefully\n",
    "            df['created_at_dt'] = pd.to_datetime(df['created_at'], errors='coerce')\n",
    "            \n",
    "            # Count failures after first attempt\n",
    "            mask_failed = df['created_at_dt'].isna()\n",
    "            print(f\"First attempt conversion failures: {mask_failed.sum()}\")\n",
    "            \n",
    "            # For failures, try explicit formats sequentially\n",
    "            formats_to_try = [\n",
    "                '%Y-%m-%d %H:%M:%S%z',                # 2023-04-13 10:06:17+00:00\n",
    "                '%Y-%m-%d %H:%M:%S.%f%z',             # 2023-04-14 07:31:51.104000+00:00\n",
    "                '%Y-%m-%dT%H:%M:%S%z',                # ISO format with timezone\n",
    "                '%Y-%m-%dT%H:%M:%S.%f%z',             # ISO format with fractional seconds\n",
    "                '%a %b %d %H:%M:%S %z %Y',            # Twitter-like format\n",
    "                '%Y-%m-%d %H:%M:%S',                  # Without timezone\n",
    "                '%Y-%m-%d'                            # Just date\n",
    "            ]\n",
    "            \n",
    "            # Try each format for the failed rows\n",
    "            for date_format in formats_to_try:\n",
    "                if mask_failed.sum() > 0:\n",
    "                    try:\n",
    "                        # Apply format only to failed rows\n",
    "                        df.loc[mask_failed, 'created_at_dt'] = pd.to_datetime(\n",
    "                            df.loc[mask_failed, 'created_at'], \n",
    "                            format=date_format,\n",
    "                            errors='coerce'\n",
    "                        )\n",
    "                        # Update mask of failed rows\n",
    "                        mask_failed = df['created_at_dt'].isna()\n",
    "                        print(f\"After trying format {date_format}: {mask_failed.sum()} failures remain\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error with format {date_format}: {e}\")\n",
    "            \n",
    "            # For any remaining failures, try one more approach - strip timezone info if present\n",
    "            if mask_failed.sum() > 0:\n",
    "                try:\n",
    "                    # Try to extract just the datetime part before the timezone for the remaining failed values\n",
    "                    failed_values = df.loc[mask_failed, 'created_at'].copy()\n",
    "                    # Extract datetime part before any + or - (timezone indicators)\n",
    "                    cleaned_values = failed_values.str.extract(r'(.*?)(?=[+-]|$)', expand=False).str.strip()\n",
    "                    df.loc[mask_failed, 'created_at_dt'] = pd.to_datetime(cleaned_values, errors='coerce')\n",
    "                    \n",
    "                    # Update mask again\n",
    "                    mask_failed = df['created_at_dt'].isna()\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in timezone stripping approach: {e}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in datetime conversion: {e}\")\n",
    "        \n",
    "        # For any remaining failures, assign a default datetime to avoid missing values\n",
    "        if 'created_at_dt' in df.columns:\n",
    "            mask_still_failed = df['created_at_dt'].isna()\n",
    "            if mask_still_failed.any():\n",
    "                print(f\"Assigning default datetime to {mask_still_failed.sum()} records that failed conversion\")\n",
    "                # Use median date as default for failed conversions\n",
    "                valid_dates = df.loc[~df['created_at_dt'].isna(), 'created_at_dt']\n",
    "                if len(valid_dates) > 0:\n",
    "                    default_date = valid_dates.median()\n",
    "                else:\n",
    "                    default_date = pd.Timestamp('2023-01-01')\n",
    "                \n",
    "                df.loc[mask_still_failed, 'created_at_dt'] = default_date\n",
    "        \n",
    "        # Keep track of conversion success rate\n",
    "        final_failed = df['created_at_dt'].isna().sum()\n",
    "        print(f\"Final datetime conversion failures: {final_failed} ({final_failed/len(df)*100:.2f}%)\")\n",
    "        \n",
    "        # Copy the datetime column to the original column name too\n",
    "        df['created_at'] = df['created_at_dt']\n",
    "    \n",
    "    # Clean up memory\n",
    "    gc.collect()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def engineer_features(df):\n",
    "    \"\"\"Extract and engineer features from the data, include behavioral features\"\"\"\n",
    "    print(\"Engineering features...\")\n",
    "    \n",
    "    # Ensure we have a working datetime column\n",
    "    if 'created_at_dt' not in df.columns and 'created_at' in df.columns:\n",
    "        df['created_at_dt'] = df['created_at']\n",
    "    \n",
    "    # Temporal features - use created_at_dt preferentially\n",
    "    datetime_col = 'created_at_dt' if 'created_at_dt' in df.columns else 'created_at'\n",
    "    \n",
    "    if datetime_col in df.columns:\n",
    "        # Ensure it's datetime type\n",
    "        if not pd.api.types.is_datetime64_any_dtype(df[datetime_col]):\n",
    "            print(f\"Converting {datetime_col} to datetime...\")\n",
    "            df[datetime_col] = pd.to_datetime(df[datetime_col], errors='coerce')\n",
    "        \n",
    "        # Only proceed with datetime operations for non-NaN values\n",
    "        mask = ~df[datetime_col].isna()\n",
    "        \n",
    "        if mask.any():  # Only calculate if there are valid datetime values\n",
    "            # Extract temporal features only for valid dates\n",
    "            df.loc[mask, 'hour_of_day'] = df.loc[mask, datetime_col].dt.hour\n",
    "            df.loc[mask, 'day_of_week'] = df.loc[mask, datetime_col].dt.dayofweek\n",
    "            df.loc[mask, 'month'] = df.loc[mask, datetime_col].dt.month\n",
    "            df.loc[mask, 'day'] = df.loc[mask, datetime_col].dt.day\n",
    "            df.loc[mask, 'year'] = df.loc[mask, datetime_col].dt.year\n",
    "            \n",
    "            # NEW: Extract minute of hour for more detailed temporal patterns\n",
    "            df.loc[mask, 'minute_of_hour'] = df.loc[mask, datetime_col].dt.minute\n",
    "            \n",
    "            # NEW: Is post during typical working hours (9am-5pm local time)\n",
    "            df.loc[mask, 'is_working_hours'] = ((df.loc[mask, 'hour_of_day'] >= 9) & \n",
    "                                               (df.loc[mask, 'hour_of_day'] < 17)).astype(int)\n",
    "            \n",
    "            # NEW: Is post during typical sleeping hours (11pm-5am local time)\n",
    "            df.loc[mask, 'is_sleeping_hours'] = ((df.loc[mask, 'hour_of_day'] >= 23) | \n",
    "                                                (df.loc[mask, 'hour_of_day'] < 5)).astype(int)\n",
    "            \n",
    "            # NEW: Is weekend post\n",
    "            df.loc[mask, 'is_weekend'] = (df.loc[mask, 'day_of_week'] >= 5).astype(int)\n",
    "        else:\n",
    "            # Create default columns if no valid dates\n",
    "            for col in ['hour_of_day', 'day_of_week', 'month', 'day', 'year', \n",
    "                       'minute_of_hour', 'is_working_hours', 'is_sleeping_hours', 'is_weekend']:\n",
    "                df[col] = 0\n",
    "    \n",
    "    # Group by author and calculate posting frequency\n",
    "    if 'author_handle' in df.columns:\n",
    "        author_post_counts = df.groupby('author_handle').size().reset_index(name='post_count')\n",
    "        df = df.merge(author_post_counts, on='author_handle', how='left')\n",
    "    \n",
    "    # Calculate time differences between posts for each author\n",
    "    if 'author_handle' in df.columns and datetime_col in df.columns:\n",
    "        # Only sort if we have valid datetime\n",
    "        if pd.api.types.is_datetime64_any_dtype(df[datetime_col]):\n",
    "            # Create a copy to avoid modifying during iteration\n",
    "            df_sorted = df.sort_values(['author_handle', datetime_col])\n",
    "            \n",
    "            # Create prev_post_time\n",
    "            df_sorted['prev_post_time'] = df_sorted.groupby('author_handle')[datetime_col].shift(1)\n",
    "            \n",
    "            # Calculate time difference safely\n",
    "            valid_times = ~df_sorted[datetime_col].isna() & ~df_sorted['prev_post_time'].isna()\n",
    "            \n",
    "            # Initialize the column with zeros\n",
    "            df_sorted['time_since_last_post'] = 0\n",
    "            \n",
    "            # Only calculate for valid rows\n",
    "            if valid_times.any():\n",
    "                df_sorted.loc[valid_times, 'time_since_last_post'] = (\n",
    "                    (df_sorted.loc[valid_times, datetime_col] - \n",
    "                     df_sorted.loc[valid_times, 'prev_post_time']).dt.total_seconds() / 60\n",
    "                )\n",
    "            \n",
    "            # Copy these columns back to the original dataframe\n",
    "            # Match by index to avoid losing data\n",
    "            df['prev_post_time'] = df_sorted['prev_post_time']\n",
    "            df['time_since_last_post'] = df_sorted['time_since_last_post']\n",
    "            \n",
    "            # NEW: Calculate posting rhythm features\n",
    "            # Group by author\n",
    "            author_groups = df.groupby('author_handle')\n",
    "            \n",
    "            # Initialize new features at author level\n",
    "            time_diff_stats = author_groups['time_since_last_post'].agg(['mean', 'std', 'median']).fillna(0)\n",
    "            time_diff_stats.columns = ['avg_time_between_posts', 'std_time_between_posts', 'median_time_between_posts']\n",
    "            \n",
    "            # Add coefficient of variation (std/mean) - more regular posting has lower values\n",
    "            time_diff_stats['cv_time_between_posts'] = np.divide(\n",
    "                time_diff_stats['std_time_between_posts'],\n",
    "                time_diff_stats['avg_time_between_posts'],\n",
    "                out=np.zeros_like(time_diff_stats['std_time_between_posts']),\n",
    "                where=time_diff_stats['avg_time_between_posts'] != 0\n",
    "            )\n",
    "            \n",
    "            # Merge back to original dataframe\n",
    "            df = df.merge(time_diff_stats, on='author_handle', how='left')\n",
    "            \n",
    "            # NEW: Calculate timing entropy for each author (more regular = more bot-like)\n",
    "            def calculate_time_entropy(times):\n",
    "                if len(times) <= 1:\n",
    "                    return 0\n",
    "                \n",
    "                # Convert to hours and bin to get distribution\n",
    "                hours = times.dt.hour\n",
    "                bins = np.bincount(hours, minlength=24)\n",
    "                probs = bins / len(hours)\n",
    "                # Filter out zeros\n",
    "                probs = probs[probs > 0]\n",
    "                return entropy(probs)\n",
    "            \n",
    "            # Apply to each author group\n",
    "            timing_entropy = author_groups[datetime_col].apply(calculate_time_entropy).fillna(0)\n",
    "            timing_entropy = timing_entropy.reset_index()\n",
    "            timing_entropy.columns = ['author_handle', 'posting_time_entropy']\n",
    "            \n",
    "            # Merge back\n",
    "            df = df.merge(timing_entropy, on='author_handle', how='left')\n",
    "            \n",
    "        else:\n",
    "            # If datetime is not proper, create dummy columns\n",
    "            df['prev_post_time'] = pd.NaT\n",
    "            df['time_since_last_post'] = 0\n",
    "            df['avg_time_between_posts'] = 0\n",
    "            df['std_time_between_posts'] = 0\n",
    "            df['median_time_between_posts'] = 0\n",
    "            df['cv_time_between_posts'] = 0\n",
    "            df['posting_time_entropy'] = 0\n",
    "\n",
    "\n",
    "    # Content features\n",
    "    if 'post_text' in df.columns:\n",
    "        df['text_length'] = df['post_text'].apply(lambda x: len(str(x)))\n",
    "        df['word_count'] = df['post_text'].apply(lambda x: len(str(x).split()))\n",
    "        \n",
    "        # Calculate entropy of text (measure of randomness)\n",
    "        df['text_entropy'] = df['post_text'].apply(calculate_entropy)\n",
    "        \n",
    "        # NEW: Calculate lexical diversity (unique words / total words)\n",
    "        def lexical_diversity(text):\n",
    "            if not isinstance(text, str) or len(text) == 0:\n",
    "                return 0\n",
    "            words = text.split()\n",
    "            if len(words) == 0:\n",
    "                return 0\n",
    "            return len(set(words)) / len(words)\n",
    "        \n",
    "        df['lexical_diversity'] = df['post_text'].apply(lexical_diversity)\n",
    "        \n",
    "        # NEW: Hashtag and mention counts\n",
    "        df['hashtag_count'] = df['post_text'].apply(lambda x: str(x).count('#') if isinstance(x, str) else 0)\n",
    "        df['mention_count'] = df['post_text'].apply(lambda x: str(x).count('@') if isinstance(x, str) else 0)\n",
    "        \n",
    "        # NEW: URL count\n",
    "        url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "        df['url_count'] = df['post_text'].apply(lambda x: len(re.findall(url_pattern, str(x))) if isinstance(x, str) else 0)\n",
    "    \n",
    "    # Engagement features\n",
    "    if all(col in df.columns for col in ['reply_count', 'repost_count', 'like_count']):\n",
    "        df['total_engagement'] = df['reply_count'] + df['repost_count'] + df['like_count']\n",
    "        \n",
    "        # NEW: Calculate engagement ratios\n",
    "        df['likes_to_reposts_ratio'] = np.divide(\n",
    "            df['like_count'], \n",
    "            df['repost_count'], \n",
    "            out=np.zeros_like(df['like_count'], dtype=float),\n",
    "            where=df['repost_count'] != 0\n",
    "        )\n",
    "        \n",
    "        df['engagement_ratio'] = np.divide(\n",
    "            df['total_engagement'],\n",
    "            df['text_length'] + 1,\n",
    "            out=np.zeros_like(df['total_engagement'], dtype=float),\n",
    "            where=(df['text_length'] + 1) != 0\n",
    "        )\n",
    "        \n",
    "        # NEW: Calculate engagement consistency for each author\n",
    "        engagement_stats = df.groupby('author_handle')['total_engagement'].agg(['mean', 'std']).fillna(0)\n",
    "        engagement_stats.columns = ['avg_engagement', 'std_engagement']\n",
    "        \n",
    "        # Calculate coefficient of variation for engagement\n",
    "        engagement_stats['cv_engagement'] = np.divide(\n",
    "            engagement_stats['std_engagement'],\n",
    "            engagement_stats['avg_engagement'],\n",
    "            out=np.zeros_like(engagement_stats['std_engagement']),\n",
    "            where=engagement_stats['avg_engagement'] != 0\n",
    "        )\n",
    "        \n",
    "        # Merge back\n",
    "        df = df.merge(engagement_stats, on='author_handle', how='left')\n",
    "    \n",
    "    # Sentiment features\n",
    "    if all(col in df.columns for col in ['happy', 'sad', 'neutral']):\n",
    "        df['sentiment_ratio'] = np.divide(\n",
    "            (df['happy'] - df['sad']),\n",
    "            (df['happy'] + df['sad'] + 1),\n",
    "            out=np.zeros_like(df['happy'], dtype=float),\n",
    "            where=(df['happy'] + df['sad'] + 1) != 0\n",
    "        )\n",
    "        \n",
    "        # NEW: Calculate sentiment consistency for each author\n",
    "        sentiment_stats = df.groupby('author_handle')['sentiment_ratio'].agg(['mean', 'std']).fillna(0)\n",
    "        sentiment_stats.columns = ['avg_sentiment', 'std_sentiment']\n",
    "        \n",
    "        # Merge back\n",
    "        df = df.merge(sentiment_stats, on='author_handle', how='left')\n",
    "    \n",
    "    # Content similarity and near-duplicate detection\n",
    "    if 'author_handle' in df.columns and 'post_text' in df.columns:\n",
    "        print(\"Calculating content similarity...\")\n",
    "        # Process in smaller groups to save memory\n",
    "        author_groups = df.groupby('author_handle')\n",
    "        similarity_results = []\n",
    "        duplicate_results = []\n",
    "        \n",
    "        for author, group in author_groups:\n",
    "            if len(group) > 1:\n",
    "                sim_series, dup_series = calculate_similarity_features(group)\n",
    "                similarity_results.append(pd.DataFrame({'author_handle': author, 'index': group.index, 'content_similarity': sim_series}))\n",
    "                duplicate_results.append(pd.DataFrame({'author_handle': author, 'index': group.index, 'has_near_duplicate': dup_series}))\n",
    "        \n",
    "        if similarity_results:\n",
    "            sim_df = pd.concat(similarity_results)\n",
    "            dup_df = pd.concat(duplicate_results)\n",
    "            \n",
    "            df = df.merge(sim_df[['index', 'content_similarity']], left_index=True, right_on='index', how='left')\n",
    "            df = df.merge(dup_df[['index', 'has_near_duplicate']], left_index=True, right_on='index', how='left')\n",
    "            \n",
    "            df.drop('index_x', axis=1, errors='ignore', inplace=True)\n",
    "            df.drop('index_y', axis=1, errors='ignore', inplace=True)\n",
    "            df.drop('index', axis=1, errors='ignore', inplace=True)\n",
    "        \n",
    "        df['content_similarity'] = df['content_similarity'].fillna(0)\n",
    "        df['has_near_duplicate'] = df['has_near_duplicate'].fillna(0)\n",
    "        \n",
    "        # NEW: Calculate self-similarity ratio for each author\n",
    "        self_sim_stats = df.groupby('author_handle')['content_similarity'].agg(['mean']).fillna(0)\n",
    "        self_sim_stats.columns = ['avg_content_similarity']\n",
    "        \n",
    "        # Merge back\n",
    "        df = df.merge(self_sim_stats, on='author_handle', how='left')\n",
    "    \n",
    "    # Clean up memory\n",
    "    gc.collect()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def calculate_entropy(text):\n",
    "    \"\"\"Calculate the entropy of text as a measure of randomness\"\"\"\n",
    "    if not isinstance(text, str) or len(text) == 0:\n",
    "        return 0\n",
    "    prob = [float(text.count(c)) / len(text) for c in set(text)]\n",
    "    entropy = -sum([p * np.log2(p) for p in prob])\n",
    "    return entropy\n",
    "\n",
    "def calculate_similarity_features(author_group, threshold=0.9):\n",
    "    \"\"\"Calculate content similarity and detect near-duplicates for posts by the same author\"\"\"\n",
    "    # Use cleaned_text specifically for similarity calculation\n",
    "    texts = author_group['cleaned_text'].tolist()\n",
    "    indices = author_group.index\n",
    "    \n",
    "    if len(texts) <= 1:\n",
    "        return pd.Series([0] * len(author_group), index=indices), pd.Series([0] * len(author_group), index=indices)\n",
    "    \n",
    "    try:\n",
    "        # Clean and validate texts\n",
    "        valid_texts = []\n",
    "        for text in texts:\n",
    "            if isinstance(text, str) and len(str(text).strip()) > 0:\n",
    "                valid_texts.append(str(text))\n",
    "            else:\n",
    "                valid_texts.append(\"empty_text\")  # Placeholder for invalid texts\n",
    "        \n",
    "        # Use TF-IDF vectorization for text comparison\n",
    "        tfidf = TfidfVectorizer(min_df=1, stop_words='english').fit_transform(valid_texts)\n",
    "        similarity_matrix = cosine_similarity(tfidf)\n",
    "        \n",
    "        # Calculate average similarity with other posts by same author\n",
    "        avg_similarities = [\n",
    "            np.mean([similarity_matrix[i][j] for j in range(len(valid_texts)) if i != j]) \n",
    "            if len(valid_texts) > 1 else 0 for i in range(len(valid_texts))\n",
    "        ]\n",
    "        \n",
    "        # Detect near-duplicates (posts with high similarity to others)\n",
    "        has_near_duplicate = [(similarity_matrix[i] > threshold).sum() > 1 for i in range(len(valid_texts))]\n",
    "        \n",
    "        return pd.Series(avg_similarities, index=indices), pd.Series(has_near_duplicate, index=indices).astype(int)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in similarity calculation: {e}\")\n",
    "        return pd.Series([0] * len(author_group), index=indices), pd.Series([0] * len(author_group), index=indices)\n",
    "\n",
    "\n",
    "\n",
    "def analyze_behavioral_patterns(df):\n",
    "    \"\"\"Analyze behavioral patterns to identify bot-like activities\"\"\"\n",
    "    print(\"Analyzing behavioral patterns...\")\n",
    "    \n",
    "    # NEW: Create user posting behavior profiles\n",
    "    if 'author_handle' in df.columns and 'created_at_dt' in df.columns:\n",
    "        # Time-based behavior patterns\n",
    "        \n",
    "        # 1. Benford's Law Analysis for time intervals\n",
    "        # Per the recent research, legitimate users often follow Benford's Law in their activity patterns\n",
    "        # while bots often deviate from it\n",
    "        def calculate_benford_deviation(intervals):\n",
    "            if len(intervals) < 10:  # Need enough data points\n",
    "                return 0\n",
    "            \n",
    "            # Get first digits\n",
    "            first_digits = [int(str(int(abs(i))).lstrip('0')[0]) if i != 0 else 0 for i in intervals]\n",
    "            \n",
    "            # Remove zeros\n",
    "            first_digits = [d for d in first_digits if d != 0]\n",
    "            \n",
    "            if len(first_digits) < 5:  # Need enough non-zero values\n",
    "                return 0\n",
    "                \n",
    "            # Count occurrences of each first digit (1-9)\n",
    "            observed_counts = np.zeros(9)\n",
    "            for digit in first_digits:\n",
    "                if 1 <= digit <= 9:  # Valid first digits\n",
    "                    observed_counts[digit-1] += 1\n",
    "            \n",
    "            # Calculate expected distribution according to Benford's Law\n",
    "            total_digits = sum(observed_counts)\n",
    "            if total_digits == 0:\n",
    "                return 0\n",
    "                \n",
    "            # Expected frequencies according to Benford's Law\n",
    "            benford_dist = np.array([np.log10(1 + 1/d) for d in range(1, 10)])\n",
    "            expected_counts = benford_dist * total_digits\n",
    "            \n",
    "            # Calculate chi-square deviation\n",
    "            chi_square = np.sum(np.divide(\n",
    "                np.square(observed_counts - expected_counts),\n",
    "                expected_counts,\n",
    "                out=np.zeros_like(observed_counts),\n",
    "                where=expected_counts != 0\n",
    "            ))\n",
    "            \n",
    "            return chi_square\n",
    "        \n",
    "        # Group by author and calculate Benford deviation\n",
    "        author_groups = df.groupby('author_handle')\n",
    "        \n",
    "        benford_results = []\n",
    "        for author, group in author_groups:\n",
    "            if len(group) >= 10:  # Need enough posts\n",
    "                # Sort by time\n",
    "                group_sorted = group.sort_values('created_at_dt')\n",
    "                \n",
    "                # Calculate intervals in seconds\n",
    "                timestamps = group_sorted['created_at_dt'].astype(np.int64) // 10**9  # Convert to seconds\n",
    "                intervals = np.diff(timestamps)\n",
    "                \n",
    "                # Calculate deviation from Benford's Law\n",
    "                benford_dev = calculate_benford_deviation(intervals)\n",
    "                \n",
    "                benford_results.append({\n",
    "                    'author_handle': author,\n",
    "                    'benford_deviation': benford_dev\n",
    "                })\n",
    "        \n",
    "        if benford_results:\n",
    "            benford_df = pd.DataFrame(benford_results)\n",
    "            df = df.merge(benford_df, on='author_handle', how='left')\n",
    "            df['benford_deviation'] = df['benford_deviation'].fillna(0)\n",
    "        else:\n",
    "            df['benford_deviation'] = 0\n",
    "        \n",
    "        # 2. NEW: Periodicity detection for each author\n",
    "        # Bots often show highly periodic behavior\n",
    "        def detect_periodicity(timestamps):\n",
    "            if len(timestamps) < 10:\n",
    "                return 0\n",
    "            \n",
    "            # Calculate intervals\n",
    "            intervals = np.diff(timestamps.astype(np.int64) // 10**9)  # Convert to seconds\n",
    "            \n",
    "            if len(intervals) < 3:\n",
    "                return 0\n",
    "                \n",
    "            # Calculate standard deviation of intervals\n",
    "            std_dev = np.std(intervals)\n",
    "            mean_interval = np.mean(intervals)\n",
    "            \n",
    "            if mean_interval == 0:\n",
    "                return 0\n",
    "                \n",
    "            # Coefficient of variation (lower means more regular/periodic)\n",
    "            cv = std_dev / mean_interval\n",
    "            \n",
    "            # Regularization factor (0 means perfectly periodic, 1 means random)\n",
    "            periodicity_factor = 1 - min(1, cv / 2)  # Normalize to 0-1 range\n",
    "            \n",
    "            return periodicity_factor\n",
    "        \n",
    "        # Calculate periodicity for each author\n",
    "        periodicity_results = []\n",
    "        for author, group in author_groups:\n",
    "            if len(group) >= 10:\n",
    "                # Sort by time\n",
    "                group_sorted = group.sort_values('created_at_dt')\n",
    "                \n",
    "                # Calculate periodicity\n",
    "                periodicity = detect_periodicity(group_sorted['created_at_dt'])\n",
    "                \n",
    "                periodicity_results.append({\n",
    "                    'author_handle': author,\n",
    "                    'posting_periodicity': periodicity\n",
    "                })\n",
    "        \n",
    "        if periodicity_results:\n",
    "            periodicity_df = pd.DataFrame(periodicity_results)\n",
    "            df = df.merge(periodicity_df, on='author_handle', how='left')\n",
    "            df['posting_periodicity'] = df['posting_periodicity'].fillna(0)\n",
    "        else:\n",
    "            df['posting_periodicity'] = 0\n",
    "    \n",
    "    # If data includes posts from multiple platforms, we can detect synchronized behavior\n",
    "    if 'platform' in df.columns and 'author_handle' in df.columns and 'created_at_dt' in df.columns:\n",
    "        print(\"Analyzing cross-platform synchronization...\")\n",
    "        \n",
    "        def calculate_cross_platform_sync(group):\n",
    "            platforms = group['platform'].unique()\n",
    "            if len(platforms) <= 1:\n",
    "                return 0  # Only one platform, no cross-platform sync\n",
    "                \n",
    "            # Calculate time differences between platforms\n",
    "            sync_scores = []\n",
    "            for i, p1 in enumerate(platforms[:-1]):\n",
    "                for p2 in platforms[i+1:]:\n",
    "                    # Get posts from each platform\n",
    "                    p1_posts = group[group['platform'] == p1]['created_at_dt']\n",
    "                    p2_posts = group[group['platform'] == p2]['created_at_dt']\n",
    "                    \n",
    "                    if len(p1_posts) < 2 or len(p2_posts) < 2:\n",
    "                        continue\n",
    "                        \n",
    "                    # Convert to arrays for faster processing\n",
    "                    p1_times = p1_posts.astype(np.int64).values // 10**9  # seconds\n",
    "                    p2_times = p2_posts.astype(np.int64).values // 10**9\n",
    "                    \n",
    "                    # Find minimum time differences between posts across platforms\n",
    "                    min_diffs = []\n",
    "                    for t1 in p1_times:\n",
    "                        min_diff = min(abs(t1 - t2) for t2 in p2_times)\n",
    "                        min_diffs.append(min_diff)\n",
    "                    \n",
    "                    # Calculate median of minimum differences (in minutes)\n",
    "                    median_diff = np.median(min_diffs) / 60\n",
    "                    \n",
    "                    # Closer to 0 means more synchronized\n",
    "                    # Transform to 0-1 scale where 1 is highly synchronized\n",
    "                    # 5 minutes or less is considered highly synchronized\n",
    "                    sync_score = max(0, 1 - (median_diff / 5))\n",
    "                    sync_scores.append(sync_score)\n",
    "            \n",
    "            return np.mean(sync_scores) if sync_scores else 0\n",
    "        \n",
    "        # Calculate cross-platform sync for each author\n",
    "        sync_results = []\n",
    "        for author, group in author_groups:\n",
    "            sync_score = calculate_cross_platform_sync(group)\n",
    "            sync_results.append({\n",
    "                'author_handle': author,\n",
    "                'cross_platform_sync': sync_score\n",
    "            })\n",
    "        \n",
    "        if sync_results:\n",
    "            sync_df = pd.DataFrame(sync_results)\n",
    "            df = df.merge(sync_df, on='author_handle', how='left')\n",
    "            df['cross_platform_sync'] = df['cross_platform_sync'].fillna(0)\n",
    "        else:\n",
    "            df['cross_platform_sync'] = 0\n",
    "            \n",
    "    # Graph-based features\n",
    "    # Building interaction network and extract network features\n",
    "    if 'author_handle' in df.columns:\n",
    "        print(\"Building interaction network and extracting graph features...\")\n",
    "        \n",
    "        # Build a graph of user interactions\n",
    "        try:\n",
    "            # Check if we have interaction data\n",
    "            has_mentions = 'mention_count' in df.columns and df['mention_count'].sum() > 0\n",
    "            has_replies = 'reply_count' in df.columns and df['reply_count'].sum() > 0\n",
    "            \n",
    "            if has_mentions or has_replies:\n",
    "                # Extract mentioned users from text if available\n",
    "                if 'post_text' in df.columns:\n",
    "                    # Extract mentions (@username) from text\n",
    "                    mention_pattern = re.compile(r'@(\\w+)')\n",
    "                    \n",
    "                    # Function to extract mentions\n",
    "                    def extract_mentions(text):\n",
    "                        if not isinstance(text, str):\n",
    "                            return []\n",
    "                        return mention_pattern.findall(text)\n",
    "                    \n",
    "                    # Apply to all posts\n",
    "                    df['mentioned_users'] = df['post_text'].apply(extract_mentions)\n",
    "                    \n",
    "                    # Build interaction graph\n",
    "                    G = nx.DiGraph()\n",
    "                    \n",
    "                    # Add nodes for all users\n",
    "                    all_users = df['author_handle'].unique()\n",
    "                    G.add_nodes_from(all_users)\n",
    "                    \n",
    "                    # Add edges for mentions\n",
    "                    for _, row in df.iterrows():\n",
    "                        source = row['author_handle']\n",
    "                        for target in row['mentioned_users']:\n",
    "                            G.add_edge(source, target, type='mention')\n",
    "                    \n",
    "                    # Calculate network metrics\n",
    "                    # 1. In-degree and out-degree\n",
    "                    in_degree = dict(G.in_degree())\n",
    "                    out_degree = dict(G.out_degree())\n",
    "                    \n",
    "                    # Convert to dataframes\n",
    "                    in_degree_df = pd.DataFrame.from_dict(in_degree, orient='index', columns=['in_degree']).reset_index()\n",
    "                    in_degree_df.columns = ['author_handle', 'in_degree']\n",
    "                    \n",
    "                    out_degree_df = pd.DataFrame.from_dict(out_degree, orient='index', columns=['out_degree']).reset_index()\n",
    "                    out_degree_df.columns = ['author_handle', 'out_degree']\n",
    "                    \n",
    "                    # Merge with main dataframe\n",
    "                    df = df.merge(in_degree_df, on='author_handle', how='left')\n",
    "                    df = df.merge(out_degree_df, on='author_handle', how='left')\n",
    "                    \n",
    "                    # Fill NaN values\n",
    "                    df['in_degree'] = df['in_degree'].fillna(0)\n",
    "                    df['out_degree'] = df['out_degree'].fillna(0)\n",
    "                    \n",
    "                    # 2. Calculate ratios - bots often have skewed ratios\n",
    "                    df['in_out_ratio'] = np.divide(\n",
    "                        df['in_degree'],\n",
    "                        df['out_degree'], \n",
    "                        out=np.zeros_like(df['in_degree'], dtype=float),\n",
    "                        where=df['out_degree'] != 0\n",
    "                    )\n",
    "                    \n",
    "                    # 3. Calculate clustering coefficient - human users tend to have higher clustering\n",
    "                    if len(G.nodes) > 0:\n",
    "                        clustering = nx.clustering(G.to_undirected())\n",
    "                        clustering_df = pd.DataFrame.from_dict(clustering, orient='index', columns=['clustering_coefficient']).reset_index()\n",
    "                        clustering_df.columns = ['author_handle', 'clustering_coefficient']\n",
    "                        \n",
    "                        # Merge with main dataframe\n",
    "                        df = df.merge(clustering_df, on='author_handle', how='left')\n",
    "                        df['clustering_coefficient'] = df['clustering_coefficient'].fillna(0)\n",
    "                    else:\n",
    "                        df['clustering_coefficient'] = 0\n",
    "                    \n",
    "                    # 4. Community detection - bots often form distinctive communities\n",
    "                    # Use a simple connected components approach for efficiency\n",
    "                    # More advanced methods like Louvain could be used for larger datasets\n",
    "                    communities = nx.weakly_connected_components(G)\n",
    "                    \n",
    "                    # Map users to communities\n",
    "                    community_mapping = {}\n",
    "                    for i, community in enumerate(communities):\n",
    "                        for user in community:\n",
    "                            community_mapping[user] = i\n",
    "                            \n",
    "                    # Create dataframe from mapping\n",
    "                    community_df = pd.DataFrame.from_dict(community_mapping, orient='index', columns=['community_id']).reset_index()\n",
    "                    community_df.columns = ['author_handle', 'community_id']\n",
    "                    \n",
    "                    # Merge with main dataframe\n",
    "                    df = df.merge(community_df, on='author_handle', how='left')\n",
    "                    df['community_id'] = df['community_id'].fillna(-1)\n",
    "                    \n",
    "                    # 5. Calculate community size for each user\n",
    "                    community_sizes = community_df.groupby('community_id').size().reset_index(name='community_size')\n",
    "                    community_df = community_df.merge(community_sizes, on='community_id')\n",
    "                    \n",
    "                    # Keep only relevant columns\n",
    "                    community_df = community_df[['author_handle', 'community_size']]\n",
    "                    \n",
    "                    # Merge with main dataframe\n",
    "                    df = df.merge(community_df, on='author_handle', how='left')\n",
    "                    df['community_size'] = df['community_size'].fillna(1)\n",
    "                    \n",
    "                    # Calculate the ratio of connections within vs. outside community\n",
    "                    # This requires more complex graph analysis, skipping for efficiency\n",
    "            else:\n",
    "                # No interaction data available\n",
    "                for col in ['in_degree', 'out_degree', 'in_out_ratio', \n",
    "                           'clustering_coefficient', 'community_id', 'community_size']:\n",
    "                    df[col] = 0\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in graph analysis: {e}\")\n",
    "            # Ensure all graph columns exist\n",
    "            for col in ['in_degree', 'out_degree', 'in_out_ratio', \n",
    "                       'clustering_coefficient', 'community_id', 'community_size']:\n",
    "                df[col] = 0\n",
    "            \n",
    "    # Additional time-series features \n",
    "    # Detect automated posting patterns using time series analysis\n",
    "    if 'author_handle' in df.columns and 'created_at_dt' in df.columns:\n",
    "        print(\"Analyzing temporal behavioral patterns...\")\n",
    "        \n",
    "        def analyze_posting_shape(timestamps):\n",
    "            \"\"\"Analyze the temporal shape of posting behavior\"\"\"\n",
    "            if len(timestamps) < 24:  # Need enough data points\n",
    "                return {\n",
    "                    'burst_ratio': 0,\n",
    "                    'shape_entropy': 0,\n",
    "                    'max_burst_size': 0\n",
    "                }\n",
    "            \n",
    "            # Convert to hours for 24-hour analysis\n",
    "            hours = pd.Series(timestamps).dt.hour\n",
    "            \n",
    "            # Create 24-hour histogram\n",
    "            hour_counts = hours.value_counts().sort_index()\n",
    "            \n",
    "            # Ensure all 24 hours are represented\n",
    "            for hour in range(24):\n",
    "                if hour not in hour_counts.index:\n",
    "                    hour_counts[hour] = 0\n",
    "            \n",
    "            hour_counts = hour_counts.sort_index()\n",
    "            \n",
    "            # 1. Calculate shape entropy\n",
    "            total_posts = hour_counts.sum()\n",
    "            if total_posts == 0:\n",
    "                return {\n",
    "                    'burst_ratio': 0,\n",
    "                    'shape_entropy': 0,\n",
    "                    'max_burst_size': 0\n",
    "                }\n",
    "                \n",
    "            probabilities = hour_counts / total_posts\n",
    "            probabilities = probabilities[probabilities > 0]  # Remove zeros\n",
    "            shape_entropy = -np.sum(probabilities * np.log2(probabilities))\n",
    "            \n",
    "            # 2. Detect posting bursts\n",
    "            # Define a burst as consecutive hours with posts > mean\n",
    "            mean_posts = hour_counts.mean()\n",
    "            bursts = []\n",
    "            current_burst = []\n",
    "            \n",
    "            for hour, count in hour_counts.items():\n",
    "                if count > mean_posts:\n",
    "                    current_burst.append((hour, count))\n",
    "                elif current_burst:\n",
    "                    bursts.append(current_burst)\n",
    "                    current_burst = []\n",
    "            \n",
    "            # Add the last burst if it exists\n",
    "            if current_burst:\n",
    "                bursts.append(current_burst)\n",
    "            \n",
    "            # Calculate burst statistics\n",
    "            max_burst_size = max([len(burst) for burst in bursts]) if bursts else 0\n",
    "            burst_posts = sum([sum(count for _, count in burst) for burst in bursts])\n",
    "            burst_ratio = burst_posts / total_posts if total_posts > 0 else 0\n",
    "            \n",
    "            return {\n",
    "                'burst_ratio': burst_ratio,\n",
    "                'shape_entropy': shape_entropy,\n",
    "                'max_burst_size': max_burst_size\n",
    "            }\n",
    "        \n",
    "        # Calculate posting shape metrics for each author\n",
    "        shape_results = []\n",
    "        for author, group in author_groups:\n",
    "            if len(group) >= 24:  # Need enough posts\n",
    "                shape_metrics = analyze_posting_shape(group['created_at_dt'])\n",
    "                shape_metrics['author_handle'] = author\n",
    "                shape_results.append(shape_metrics)\n",
    "        \n",
    "        if shape_results:\n",
    "            shape_df = pd.DataFrame(shape_results)\n",
    "            df = df.merge(shape_df, on='author_handle', how='left')\n",
    "            \n",
    "            # Fill NaN values\n",
    "            for col in ['burst_ratio', 'shape_entropy', 'max_burst_size']:\n",
    "                df[col] = df[col].fillna(0)\n",
    "        else:\n",
    "            for col in ['burst_ratio', 'shape_entropy', 'max_burst_size']:\n",
    "                df[col] = 0\n",
    "    \n",
    "    # Calculate posting regularity\n",
    "    if 'author_handle' in df.columns and 'created_at_dt' in df.columns:\n",
    "        # Use created_at_dt column\n",
    "        posting_regularity = df.groupby('author_handle')['created_at_dt'].apply(\n",
    "            lambda x: calculate_posting_regularity(x) if len(x) > 1 else 0\n",
    "        )\n",
    "        df = df.merge(posting_regularity.reset_index(name='posting_regularity'), on='author_handle', how='left')\n",
    "        \n",
    "        # Coordination detection\n",
    "        # Detect coordinated posting activity across multiple users\n",
    "        print(\"Detecting coordinated behavior patterns...\")\n",
    "        \n",
    "        # Group users by posting patterns\n",
    "        \n",
    "        # 1. Create a signature for each user's posting pattern using binned activity\n",
    "        def create_temporal_signature(group, bin_width_minutes=60):\n",
    "            # Sort by timestamp\n",
    "            sorted_group = group.sort_values('created_at_dt')\n",
    "            \n",
    "            if len(sorted_group) < 5:  # Need enough posts\n",
    "                return None\n",
    "                \n",
    "            # Get timestamps as unix time (seconds)\n",
    "            timestamps = sorted_group['created_at_dt'].astype(np.int64) // 10**9\n",
    "            \n",
    "            # Calculate intervals (in minutes)\n",
    "            intervals = np.diff(timestamps) / 60\n",
    "            \n",
    "            # Bin intervals\n",
    "            bins = np.arange(0, 24*60+bin_width_minutes, bin_width_minutes)  # 24 hours of bins\n",
    "            hist, _ = np.histogram(intervals, bins=bins)\n",
    "            \n",
    "            # Normalize\n",
    "            if hist.sum() > 0:\n",
    "                hist = hist / hist.sum()\n",
    "                \n",
    "            return hist\n",
    "            \n",
    "        # Create temporal signatures for users with enough posts\n",
    "        user_signatures = {}\n",
    "        for author, group in author_groups:\n",
    "            if len(group) >= 5:\n",
    "                signature = create_temporal_signature(group)\n",
    "                if signature is not None:\n",
    "                    user_signatures[author] = signature\n",
    "        \n",
    "        # Calculate coordination scores between user pairs\n",
    "        if len(user_signatures) > 1:\n",
    "            coordination_scores = defaultdict(float)\n",
    "            \n",
    "            # Limit number of comparisons for very large datasets\n",
    "            max_users_to_compare = 5000\n",
    "            user_list = list(user_signatures.keys())\n",
    "            if len(user_list) > max_users_to_compare:\n",
    "                # Sample users for comparison\n",
    "                import random\n",
    "                random.seed(42)  # For reproducibility\n",
    "                user_list = random.sample(user_list, max_users_to_compare)\n",
    "            \n",
    "            for i, user1 in enumerate(user_list[:-1]):\n",
    "                sig1 = user_signatures[user1]\n",
    "                for user2 in user_list[i+1:]:\n",
    "                    sig2 = user_signatures[user2]\n",
    "                    \n",
    "                    # Calculate cosine similarity between temporal signatures\n",
    "                    similarity = np.sum(sig1 * sig2) / (np.sqrt(np.sum(sig1**2)) * np.sqrt(np.sum(sig2**2)) + 1e-10)\n",
    "                    \n",
    "                    # Store coordination score for both users\n",
    "                    coordination_scores[user1] = max(coordination_scores[user1], similarity)\n",
    "                    coordination_scores[user2] = max(coordination_scores[user2], similarity)\n",
    "            \n",
    "            # Convert to dataframe\n",
    "            coordination_df = pd.DataFrame([\n",
    "                {'author_handle': user, 'coordination_score': score}\n",
    "                for user, score in coordination_scores.items()\n",
    "            ])\n",
    "            \n",
    "            # Merge with main dataframe\n",
    "            df = df.merge(coordination_df, on='author_handle', how='left')\n",
    "            df['coordination_score'] = df['coordination_score'].fillna(0)\n",
    "        else:\n",
    "            df['coordination_score'] = 0\n",
    "    \n",
    "    return df\n",
    "\n",
    "def calculate_posting_regularity(times):\n",
    "    \"\"\"Calculate the regularity of posting times (lower coefficient of variation means more regular)\"\"\"\n",
    "    if len(times) <= 1 or times.isna().all():\n",
    "        return 0\n",
    "    \n",
    "    # Filter out NaN values\n",
    "    times = times.dropna()\n",
    "    if len(times) <= 1:\n",
    "        return 0\n",
    "    \n",
    "    try:\n",
    "        # Calculate time differences between consecutive posts\n",
    "        time_diffs = np.diff(times.astype(np.int64)) / 10**9  # Convert to seconds\n",
    "        \n",
    "        # Calculate coefficient of variation (lower means more regular)\n",
    "        if len(time_diffs) == 0 or np.mean(time_diffs) == 0:\n",
    "            return 0\n",
    "        \n",
    "        cv = np.std(time_diffs) / np.mean(time_diffs)\n",
    "        return cv if not np.isnan(cv) and not np.isinf(cv) else 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating posting regularity: {e}\")\n",
    "        return 0\n",
    "\n",
    "def detect_anomalies(df, features):\n",
    "    \"\"\"Use unsupervised learning to detect anomalies in the data\"\"\"\n",
    "    print(\"Detecting anomalies...\")\n",
    "    \n",
    "    try:\n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        X = scaler.fit_transform(df[features].fillna(0))\n",
    "        \n",
    "        # Use Isolation Forest for anomaly detection\n",
    "        isolation_forest = IsolationForest(\n",
    "            n_estimators=100, \n",
    "            contamination=0.1,  # Assume 10% of data points are anomalies\n",
    "            max_samples='auto',\n",
    "            random_state=42\n",
    "        )\n",
    "        df['anomaly_score'] = isolation_forest.fit_predict(X)\n",
    "        df['is_anomaly'] = (df['anomaly_score'] == -1).astype(int)\n",
    "        \n",
    "        # Use DBSCAN for density-based clustering\n",
    "        try:\n",
    "            # First try with default sklearn parameters\n",
    "            dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "            df['cluster'] = dbscan.fit_predict(X)\n",
    "        except TypeError as e:\n",
    "            # If that fails, try older sklearn version compatibility\n",
    "            print(f\"DBSCAN parameter error: {e}. Trying alternative parameters.\")\n",
    "            dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "            df['cluster'] = dbscan.fit_predict(X)\n",
    "        \n",
    "        # Use K-means to identify unusual clusters\n",
    "        # Cap the number of clusters to prevent errors with small datasets\n",
    "        num_clusters = min(8, len(df))\n",
    "        if num_clusters < 2:\n",
    "            num_clusters = 2  # Minimum of 2 clusters required\n",
    "        \n",
    "        try:\n",
    "            # First try with default sklearn parameters\n",
    "            kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "            df['kmeans_cluster'] = kmeans.fit_predict(X)\n",
    "        except TypeError as e:\n",
    "            # If that fails, try older sklearn version compatibility\n",
    "            print(f\"KMeans parameter error: {e}. Trying alternative parameters.\")\n",
    "            kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "            df['kmeans_cluster'] = kmeans.fit_predict(X)\n",
    "        \n",
    "        # Count instances in each cluster\n",
    "        cluster_counts = df['kmeans_cluster'].value_counts()\n",
    "        # Consider clusters with less than 5% of the data as small\n",
    "        small_clusters = cluster_counts[cluster_counts < len(df) * 0.05].index\n",
    "        \n",
    "        # Mark as suspicious if in a small cluster\n",
    "        df['in_small_cluster'] = df['kmeans_cluster'].isin(small_clusters).astype(int)\n",
    "        \n",
    "        # Use PCA to identify outliers in the principal component space\n",
    "        if len(df) > 10 and len(features) > 1:\n",
    "            # Reduce dimensions for efficiency\n",
    "            n_components = min(5, len(features), len(df) - 1)\n",
    "            if n_components > 0:  # Ensure positive number of components\n",
    "                pca = PCA(n_components=n_components)\n",
    "                pca_result = pca.fit_transform(X)\n",
    "                \n",
    "                # Mahalanobis distance calculation\n",
    "                mu = np.mean(pca_result, axis=0)\n",
    "                \n",
    "                try:\n",
    "                    # Using covariance calculation with regularization\n",
    "                    cov = np.cov(pca_result, rowvar=False)\n",
    "                    # Add small constant to diagonal to ensure positive definiteness\n",
    "                    cov += np.eye(cov.shape[0]) * 1e-6\n",
    "                    inv_cov = np.linalg.inv(cov)\n",
    "                    \n",
    "                    # Vectorized calculation for speed\n",
    "                    mahalanobis_dist = []\n",
    "                    for i in range(len(pca_result)):\n",
    "                        delta = pca_result[i] - mu\n",
    "                        dist = np.sqrt(delta.dot(inv_cov).dot(delta))\n",
    "                        mahalanobis_dist.append(dist)\n",
    "                    \n",
    "                except np.linalg.LinAlgError:\n",
    "                    # Fallback for singular matrix - use Euclidean distance\n",
    "                    print(\"Singular matrix in Mahalanobis calculation. Using Euclidean distance.\")\n",
    "                    mahalanobis_dist = []\n",
    "                    for i in range(len(pca_result)):\n",
    "                        dist = np.linalg.norm(pca_result[i] - mu)\n",
    "                        mahalanobis_dist.append(dist)\n",
    "                \n",
    "                df['mahalanobis_dist'] = mahalanobis_dist\n",
    "                \n",
    "                # Define outliers as points with distance > 3 standard deviations\n",
    "                threshold = np.mean(mahalanobis_dist) + 3 * np.std(mahalanobis_dist)\n",
    "                df['pca_outlier'] = (df['mahalanobis_dist'] > threshold).astype(int)\n",
    "            else:\n",
    "                df['mahalanobis_dist'] = 0\n",
    "                df['pca_outlier'] = 0\n",
    "        else:\n",
    "            df['mahalanobis_dist'] = 0\n",
    "            df['pca_outlier'] = 0\n",
    "        \n",
    "        # Generate pseudo-labels based on combined anomaly detection\n",
    "        df['pseudo_label'] = 0  # Default: human\n",
    "        \n",
    "        # Mark as potential bots if any detection method flags them\n",
    "        df.loc[(df['is_anomaly'] == 1) | \n",
    "               (df['in_small_cluster'] == 1) | \n",
    "               (df['pca_outlier'] == 1), 'pseudo_label'] = 1\n",
    "        \n",
    "        # Additional rule for users in the DBSCAN noise cluster (-1) with high Mahalanobis distance\n",
    "        if 'mahalanobis_dist' in df.columns and df['mahalanobis_dist'].max() > 0:\n",
    "            threshold = df['mahalanobis_dist'].mean() + 2 * df['mahalanobis_dist'].std()\n",
    "            df.loc[(df['cluster'] == -1) & \n",
    "                   (df['mahalanobis_dist'] > threshold * 0.8), 'pseudo_label'] = 1\n",
    "    \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"Error in anomaly detection: {e}\")\n",
    "        print(traceback.format_exc())\n",
    "        # Ensure we still have the necessary columns even if the process fails\n",
    "        df['pseudo_label'] = df.get('pseudo_label', 0)\n",
    "        if 'is_anomaly' not in df.columns:\n",
    "            df['is_anomaly'] = 0\n",
    "    \n",
    "    # Clean up memory\n",
    "    gc.collect()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def train_ensemble_classifier(df, features):\n",
    "    \"\"\"Train an ensemble classifier to detect bots using advanced methods\"\"\"\n",
    "    print(\"Training ensemble classifier...\")\n",
    "    \n",
    "    try:\n",
    "        # Prepare final dataset\n",
    "        X_final = df[features].fillna(0)\n",
    "        y_final = df['pseudo_label']\n",
    "        \n",
    "        # Check if we have enough data for meaningful split\n",
    "        if len(df) < 10 or len(np.unique(y_final)) < 2:\n",
    "            print(\"Warning: Dataset too small or lacks class diversity for proper training.\")\n",
    "            # Simple fallback: assign probability based on anomaly detection\n",
    "            df['bot_probability'] = df['is_anomaly'].astype(float)\n",
    "            df['is_bot'] = df['is_anomaly']\n",
    "            return df, None\n",
    "        \n",
    "        # Split data - adaptive test size based on dataset size\n",
    "        # For very large datasets, use smaller test split to save memory\n",
    "        if len(df) > 500000:\n",
    "            test_size = 0.05\n",
    "        elif len(df) > 100000:\n",
    "            test_size = 0.1\n",
    "        else:\n",
    "            test_size = 0.2\n",
    "            \n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_final, y_final, test_size=test_size, random_state=42, stratify=y_final if len(np.unique(y_final)) > 1 else None\n",
    "        )\n",
    "        \n",
    "        # Optimize classifier parameters based on dataset size\n",
    "        if len(X_train) > 100000:\n",
    "            # For very large datasets, use fewer trees but ensure enough for accuracy\n",
    "            n_estimators = max(50, min(100, int(len(X_train) / 10000)))\n",
    "            max_depth = min(8, max(3, int(np.log2(len(X_train)))))\n",
    "            min_samples_split = max(5, min(20, int(len(X_train) / 50000)))\n",
    "        else:\n",
    "            # For smaller datasets, use more trees for better accuracy\n",
    "            n_estimators = 100\n",
    "            max_depth = min(10, max(3, int(np.log2(len(X_train)) + 2)))\n",
    "            min_samples_split = 5\n",
    "        \n",
    "        print(f\"Training with {n_estimators} trees, max_depth={max_depth}\")\n",
    "        \n",
    "        # Using ExtraTreesClassifier with optimized parameters\n",
    "        try:\n",
    "            et_classifier = ExtraTreesClassifier(\n",
    "                n_estimators=n_estimators,\n",
    "                max_depth=max_depth,\n",
    "                min_samples_split=min_samples_split,\n",
    "                class_weight='balanced',\n",
    "                random_state=42\n",
    "            )\n",
    "            \n",
    "            # Add batch training for very large datasets to avoid memory issues\n",
    "            if len(X_train) > 500000:\n",
    "                # Train in batches\n",
    "                batch_size = 100000\n",
    "                for i in range(0, len(X_train), batch_size):\n",
    "                    end = min(i + batch_size, len(X_train))\n",
    "                    print(f\"Training batch {i//batch_size + 1}/{(len(X_train) + batch_size - 1)//batch_size}\")\n",
    "                    if i == 0:\n",
    "                        # First batch - fit the classifier\n",
    "                        et_classifier.fit(X_train[i:end], y_train[i:end])\n",
    "                    else:\n",
    "                        # Subsequent batches - can't easily do partial_fit with trees, \n",
    "                        # so we'll just use the first batch for large datasets\n",
    "                        pass\n",
    "            else:\n",
    "                # Standard training for normal-sized datasets\n",
    "                et_classifier.fit(X_train, y_train)\n",
    "            \n",
    "            # Make predictions\n",
    "            y_pred = et_classifier.predict(X_test)\n",
    "            y_prob = et_classifier.predict_proba(X_test)[:, 1]\n",
    "            \n",
    "            # Evaluate\n",
    "            print(classification_report(y_test, y_pred))\n",
    "            print(confusion_matrix(y_test, y_pred))\n",
    "            \n",
    "            # Apply to full dataset, using batches for very large datasets\n",
    "            if len(X_final) > 500000:\n",
    "                bot_probs = []\n",
    "                batch_size = 100000\n",
    "                for i in range(0, len(X_final), batch_size):\n",
    "                    end = min(i + batch_size, len(X_final))\n",
    "                    batch_probs = et_classifier.predict_proba(X_final[i:end])[:, 1]\n",
    "                    bot_probs.extend(batch_probs)\n",
    "                df['bot_probability'] = bot_probs\n",
    "            else:\n",
    "                df['bot_probability'] = et_classifier.predict_proba(X_final)[:, 1]\n",
    "            \n",
    "            # Find optimal threshold using precision-recall curve\n",
    "            precision, recall, thresholds = precision_recall_curve(y_test, y_prob)\n",
    "            f1_scores = 2 * recall * precision / (recall + precision + 1e-10)\n",
    "            \n",
    "            # Handle edge case where f1_scores might be all NaN\n",
    "            if np.all(np.isnan(f1_scores)):\n",
    "                optimal_threshold = 0.5\n",
    "            else:\n",
    "                optimal_idx = np.argmax(f1_scores)\n",
    "                if len(thresholds) > optimal_idx:\n",
    "                    optimal_threshold = thresholds[optimal_idx]\n",
    "                else:\n",
    "                    optimal_threshold = 0.5\n",
    "            \n",
    "            print(f\"Optimal threshold: {optimal_threshold}\")\n",
    "            \n",
    "            df['is_bot'] = (df['bot_probability'] > optimal_threshold).astype(int)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in ExtraTreesClassifier: {e}\")\n",
    "            # Fallback to a simpler classifier if ExtraTrees fails\n",
    "            print(\"Falling back to RandomForestClassifier\")\n",
    "            rf_classifier = RandomForestClassifier(\n",
    "                n_estimators=min(50, n_estimators),\n",
    "                max_depth=max_depth,\n",
    "                min_samples_split=min_samples_split,\n",
    "                class_weight='balanced',\n",
    "                random_state=42\n",
    "            )\n",
    "            rf_classifier.fit(X_train, y_train)\n",
    "            df['bot_probability'] = rf_classifier.predict_proba(X_final)[:, 1]\n",
    "            df['is_bot'] = (df['bot_probability'] > 0.5).astype(int)\n",
    "            et_classifier = rf_classifier  # Return the RF as the classifier\n",
    "    \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"Error in ensemble training: {e}\")\n",
    "        print(traceback.format_exc())\n",
    "        # Ensure we have the necessary columns even if training fails\n",
    "        if 'bot_probability' not in df.columns:\n",
    "            print(\"Training failed. Using anomaly scores as fallback.\")\n",
    "            df['bot_probability'] = df['is_anomaly'].astype(float)\n",
    "        if 'is_bot' not in df.columns:\n",
    "            df['is_bot'] = df['is_anomaly']\n",
    "        et_classifier = None\n",
    "    \n",
    "    # Clean up memory\n",
    "    gc.collect()\n",
    "    \n",
    "    return df, et_classifier\n",
    "\n",
    "\n",
    "def aggregate_results(df, optimal_threshold):\n",
    "    \"\"\"Aggregated results by author to get author-level bot probabilities with consistent classification\"\"\"\n",
    "    print(\"Aggregating results by author...\")\n",
    "    \n",
    "    try:\n",
    "        # Check if author_handle column exists\n",
    "        if 'author_handle' not in df.columns:\n",
    "            if 'author_did' in df.columns:\n",
    "                df['author_handle'] = df['author_did']  # Use author_did as fallback\n",
    "                print(\"Using 'author_did' column as author identifier\")\n",
    "            else:\n",
    "                # Create dummy author column\n",
    "                print(\"No author identifier found. Creating dummy identifiers.\")\n",
    "                df['author_handle'] = [f\"user_{i}\" for i in range(len(df))]\n",
    "        \n",
    "        # Filter to users with 2+ posts\n",
    "        post_counts = df['author_handle'].value_counts()\n",
    "        users_with_multiple_posts = post_counts[post_counts >= 2].index\n",
    "        \n",
    "        # Filter the dataframe to include only these users\n",
    "        df_filtered = df[df['author_handle'].isin(users_with_multiple_posts)].reset_index(drop=True)\n",
    "        \n",
    "        # If no users have multiple posts, use the original dataset\n",
    "        if len(df_filtered) == 0:\n",
    "            print(\"Warning: No users with 2+ posts found. Using all users.\")\n",
    "            df_filtered = df\n",
    "        else:\n",
    "            print(f\"Found {len(users_with_multiple_posts)} users with 2+ posts\")\n",
    "        \n",
    "        # Aggregate results by author\n",
    "        author_results = df_filtered.groupby('author_handle')['bot_probability'].mean().reset_index()\n",
    "        \n",
    "        # Add post count information\n",
    "        author_post_counts = df_filtered.groupby('author_handle').size().reset_index(name='post_count')\n",
    "        author_results = author_results.merge(author_post_counts, on='author_handle', how='left')\n",
    "        \n",
    "        # Add confidence score based on post count (more posts = higher confidence)\n",
    "        author_results['confidence'] = 1 - (1 / (1 + np.log1p(author_results['post_count'])))\n",
    "        \n",
    "        # Use the optimal threshold on author-level bot probability\n",
    "        author_results['is_bot'] = (author_results['bot_probability'] > optimal_threshold).astype(int)\n",
    "        \n",
    "        # Add additional metrics if available\n",
    "        for metric in ['content_similarity', 'posting_regularity', 'benford_deviation', \n",
    "                      'coordination_score', 'posting_periodicity', 'burst_ratio']:\n",
    "            if metric in df_filtered.columns:\n",
    "                metric_agg = df_filtered.groupby('author_handle')[metric].mean().reset_index()\n",
    "                author_results = author_results.merge(metric_agg, on='author_handle', how='left')\n",
    "        \n",
    "        # Add validation to check for inconsistencies\n",
    "        if 'is_bot' in df.columns:\n",
    "            # Calculate the original model-based classification for comparison\n",
    "            is_bot_model = df_filtered.groupby('author_handle')['is_bot'].agg(\n",
    "                lambda x: x.value_counts().index[0] if len(x) > 0 else 0\n",
    "            ).reset_index()\n",
    "            is_bot_model.columns = ['author_handle', 'is_bot_model']\n",
    "            \n",
    "            # Merge with results\n",
    "            temp_results = author_results.merge(is_bot_model, on='author_handle', how='left')\n",
    "            \n",
    "            # Check for mismatches\n",
    "            mismatches = (temp_results['is_bot'] != temp_results['is_bot_model']).sum()\n",
    "        \n",
    "        print(f\"Final bot detection results: {author_results['is_bot'].sum()} bots out of {len(author_results)} total accounts\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"Error in result aggregation: {e}\")\n",
    "        print(traceback.format_exc())\n",
    "        \n",
    "        # Create a minimal result set in case of errors\n",
    "        if 'author_handle' in df.columns and 'bot_probability' in df.columns:\n",
    "            print(\"Creating simplified results due to aggregation error\")\n",
    "            author_results = df.groupby('author_handle')['bot_probability'].mean().reset_index()\n",
    "            author_results['is_bot'] = (author_results['bot_probability'] > optimal_threshold).astype(int)\n",
    "            author_results['post_count'] = df.groupby('author_handle').size().reset_index(name='count')['count']\n",
    "            author_results['confidence'] = 0.5  # Default confidence\n",
    "        else:\n",
    "            print(\"Cannot create valid results due to missing data\")\n",
    "            # Create dummy results\n",
    "            author_results = pd.DataFrame({\n",
    "                'author_handle': ['error_state'],\n",
    "                'bot_probability': [0.0],\n",
    "                'is_bot': [0],\n",
    "                'post_count': [0],\n",
    "                'confidence': [0.0]\n",
    "            })\n",
    "    \n",
    "    return author_results\n",
    "\n",
    "\n",
    "def validate_and_save_results(author_results, output_file, optimal_threshold):\n",
    "    \"\"\"Validate and save the results to ensure consistency\"\"\"\n",
    "    print(\"Validating and saving results...\")\n",
    "    \n",
    "    # Final validation check\n",
    "    mismatch_count = ((author_results['bot_probability'] > optimal_threshold) != \n",
    "                      (author_results['is_bot'] == 1)).sum()\n",
    "    \n",
    "    if mismatch_count > 0:\n",
    "        print(f\"ERROR: {mismatch_count} accounts have inconsistent is_bot classifications!\")\n",
    "        print(\"Fixing inconsistencies before saving...\")\n",
    "        \n",
    "        # Fix inconsistencies\n",
    "        author_results['is_bot'] = (author_results['bot_probability'] > optimal_threshold).astype(int)\n",
    "    \n",
    "    # Save results\n",
    "    author_results.to_csv(output_file, index=False)\n",
    "    print(f\"Results validated and saved to {output_file}\")\n",
    "    print(f\"Detected {author_results['is_bot'].sum()} bot accounts out of {len(author_results)} total accounts\")\n",
    "\n",
    "def visualize_results(df, author_results, classifier=None, features=None):\n",
    "    \"\"\"Visualize the results of bot detection with improved visualizations and error handling\"\"\"\n",
    "    print(\"Generating visualizations...\")\n",
    "    \n",
    "    try:\n",
    "        # 1. Distribution of bot probabilities\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(df['bot_probability'], bins=50, kde=True)\n",
    "        plt.title('Distribution of Bot Probabilities')\n",
    "        plt.xlabel('Bot Probability')\n",
    "        plt.ylabel('Count')\n",
    "        plt.savefig('bot_probability_distribution.png')\n",
    "        plt.close()\n",
    "        \n",
    "        # 2. Feature importance - only if we have a classifier and features\n",
    "        if classifier is not None and hasattr(classifier, 'feature_importances_') and features is not None:\n",
    "            # Verify feature_importances_ has the right length\n",
    "            if len(classifier.feature_importances_) == len(features):\n",
    "                plt.figure(figsize=(12, 8))\n",
    "                feature_importance = pd.DataFrame({\n",
    "                    'Feature': features,\n",
    "                    'Importance': classifier.feature_importances_\n",
    "                }).sort_values('Importance', ascending=False)\n",
    "                \n",
    "                # Only plot top 20 or fewer if less are available\n",
    "                top_n = min(20, len(feature_importance))\n",
    "                sns.barplot(x='Importance', y='Feature', data=feature_importance.head(top_n))\n",
    "                plt.title(f'Top {top_n} Feature Importances')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig('feature_importance.png')\n",
    "                plt.close()\n",
    "            else:\n",
    "                print(f\"Feature importance array length ({len(classifier.feature_importances_)}) doesn't match feature count ({len(features)})\")\n",
    "        \n",
    "        # 3. Bot vs Human comparison\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        bot_authors = author_results[author_results['is_bot'] == 1]['author_handle'].values\n",
    "        \n",
    "        if len(bot_authors) > 0:\n",
    "            bot_data = df[df['author_handle'].isin(bot_authors)]\n",
    "            human_data = df[~df['author_handle'].isin(bot_authors)]\n",
    "            \n",
    "            # Select comparison features that exist in the dataframe\n",
    "            potential_features = ['post_count', 'text_length', 'engagement_ratio', 'content_similarity', \n",
    "                                'posting_periodicity', 'benford_deviation']\n",
    "            comparison_features = [f for f in potential_features if f in df.columns]\n",
    "            \n",
    "            # Limit to at most 6 features for visualization clarity\n",
    "            comparison_features = comparison_features[:6]\n",
    "            \n",
    "            if len(comparison_features) > 0:\n",
    "                for i, feature in enumerate(comparison_features):\n",
    "                    if i < len(comparison_features):  # Ensure we don't exceed comparison features count\n",
    "                        plt.subplot(2, (len(comparison_features) + 1) // 2, i+1)\n",
    "                        \n",
    "                        # Ensure there's data to plot\n",
    "                        if len(bot_data) > 0 and feature in bot_data.columns:\n",
    "                            bot_values = bot_data[feature].fillna(0).values\n",
    "                            if len(bot_values) > 0 and not np.all(np.isnan(bot_values)):\n",
    "                                sns.kdeplot(bot_values, label='Bot')\n",
    "                                \n",
    "                        if len(human_data) > 0 and feature in human_data.columns:\n",
    "                            human_values = human_data[feature].fillna(0).values\n",
    "                            if len(human_values) > 0 and not np.all(np.isnan(human_values)):\n",
    "                                sns.kdeplot(human_values, label='Human')\n",
    "                                \n",
    "                        plt.title(f'{feature} Distribution')\n",
    "                        plt.legend()\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.savefig('bot_human_comparison.png')\n",
    "            else:\n",
    "                print(\"No valid comparison features found\")\n",
    "        else:\n",
    "            print(\"No bot authors found for comparison visualization\")\n",
    "            \n",
    "        plt.close()\n",
    "        \n",
    "        # 4. NEW: Temporal activity heatmap (hour of day vs day of week)\n",
    "        if 'hour_of_day' in df.columns and 'day_of_week' in df.columns and 'author_handle' in df.columns:\n",
    "            try:\n",
    "                plt.figure(figsize=(12, 8))\n",
    "                \n",
    "                # Compare hour of day vs day of week for bots vs humans\n",
    "                if len(bot_authors) > 0:\n",
    "                    # Create pivot tables for heatmaps\n",
    "                    bot_pivot = pd.crosstab(\n",
    "                        df[df['author_handle'].isin(bot_authors)]['day_of_week'],\n",
    "                        df[df['author_handle'].isin(bot_authors)]['hour_of_day'],\n",
    "                        normalize='all'\n",
    "                    )\n",
    "                    \n",
    "                    human_pivot = pd.crosstab(\n",
    "                        df[~df['author_handle'].isin(bot_authors)]['day_of_week'],\n",
    "                        df[~df['author_handle'].isin(bot_authors)]['hour_of_day'],\n",
    "                        normalize='all'\n",
    "                    )\n",
    "                    \n",
    "                    # Plot bot heatmap\n",
    "                    plt.subplot(2, 1, 1)\n",
    "                    sns.heatmap(bot_pivot, cmap='Reds', vmin=0, vmax=None)\n",
    "                    plt.title('Bot Posting Activity (Day vs Hour)')\n",
    "                    plt.xlabel('Hour of Day (0-23)')\n",
    "                    plt.ylabel('Day of Week (0=Mon, 6=Sun)')\n",
    "                    \n",
    "                    # Plot human heatmap\n",
    "                    plt.subplot(2, 1, 2)\n",
    "                    sns.heatmap(human_pivot, cmap='Blues', vmin=0, vmax=None)\n",
    "                    plt.title('Human Posting Activity (Day vs Hour)')\n",
    "                    plt.xlabel('Hour of Day (0-23)')\n",
    "                    plt.ylabel('Day of Week (0=Mon, 6=Sun)')\n",
    "                    \n",
    "                    plt.tight_layout()\n",
    "                    plt.savefig('posting_time_heatmap.png')\n",
    "                    plt.close()\n",
    "            except Exception as e:\n",
    "                print(f\"Error creating temporal heatmap: {e}\")\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"Error in visualization: {e}\")\n",
    "        print(traceback.format_exc())\n",
    "    \n",
    "    return\n",
    "\n",
    "def bot_detection_pipeline(merged_df, output_file):\n",
    "    \"\"\"Complete bot detection pipeline\"\"\"\n",
    "    print(\"Starting enhanced bot detection pipeline...\")\n",
    "    \n",
    "    try:\n",
    "        # Preprocess data (filtering to users with 2+ posts is done here)\n",
    "        df = preprocess_data(merged_df)\n",
    "        \n",
    "        # Verify we have enough data to proceed\n",
    "        if len(df) == 0:\n",
    "            print(\"Error: No data remaining after preprocessing\")\n",
    "            return None, merged_df\n",
    "            \n",
    "        # Check if we have enough users with 2+ posts\n",
    "        authors = df['author_handle'].unique()\n",
    "        post_counts = df['author_handle'].value_counts()\n",
    "        multi_post_authors = post_counts[post_counts >= 2].index\n",
    "        print(f\"Found {len(authors)} unique authors, {len(multi_post_authors)} with 2+ posts\")\n",
    "        \n",
    "        if len(multi_post_authors) < 10:\n",
    "            print(\"Warning: Very few users with 2+ posts. Results may be less reliable.\")\n",
    "        \n",
    "        # Engineer features\n",
    "        df = engineer_features(df)\n",
    "        \n",
    "        # Analyze behavioral patterns\n",
    "        df = analyze_behavioral_patterns(df)\n",
    "        \n",
    "        # Define features for model, including all advanced features\n",
    "        all_features = [\n",
    "            # Basic features\n",
    "            'post_count', 'time_since_last_post', 'text_length', 'word_count',\n",
    "            'text_entropy', 'engagement_ratio', 'content_similarity',\n",
    "            'has_near_duplicate', 'hour_of_day', 'day_of_week',\n",
    "            'reply_count', 'repost_count', 'like_count', 'happy', 'sad', 'neutral',\n",
    "            'posting_regularity',\n",
    "            \n",
    "            # Advanced temporal features\n",
    "            'avg_time_between_posts', 'std_time_between_posts', 'median_time_between_posts',\n",
    "            'cv_time_between_posts', 'posting_time_entropy', 'is_working_hours',\n",
    "            'is_sleeping_hours', 'is_weekend', 'posting_periodicity',\n",
    "            'benford_deviation', 'burst_ratio', 'shape_entropy', 'max_burst_size',\n",
    "            \n",
    "            # Content features\n",
    "            'lexical_diversity', 'hashtag_count', 'mention_count', 'url_count',\n",
    "            \n",
    "            # Engagement features\n",
    "            'likes_to_reposts_ratio', 'avg_engagement', 'std_engagement', 'cv_engagement',\n",
    "            \n",
    "            # Sentiment features\n",
    "            'sentiment_ratio', 'avg_sentiment', 'std_sentiment',\n",
    "            \n",
    "            # Coordination features\n",
    "            'coordination_score', 'avg_content_similarity',\n",
    "            \n",
    "            # Network features\n",
    "            'in_degree', 'out_degree', 'in_out_ratio', 'clustering_coefficient',\n",
    "            'community_size',\n",
    "        ]\n",
    "        \n",
    "        # Filter to only include features that exist in the dataframe\n",
    "        features = [f for f in all_features if f in df.columns]\n",
    "        \n",
    "        # OPTIMIZATION: Check for highly correlated features to remove redundancy\n",
    "        if len(features) > 10:\n",
    "            try:\n",
    "                # Calculate correlation matrix\n",
    "                corr_matrix = df[features].corr().abs()\n",
    "                \n",
    "                # Create a mask for the upper triangle\n",
    "                upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "                \n",
    "                # Find features with correlation > 0.95\n",
    "                to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "                \n",
    "                if len(to_drop) > 0:\n",
    "                    print(f\"Removing {len(to_drop)} highly correlated features: {to_drop}\")\n",
    "                    features = [f for f in features if f not in to_drop]\n",
    "            except Exception as e:\n",
    "                print(f\"Error in correlation analysis: {e}\")\n",
    "                # Continue with original features\n",
    "        \n",
    "        print(f\"Using {len(features)} features for model training\")\n",
    "        \n",
    "        # Detect anomalies\n",
    "        df = detect_anomalies(df, features)\n",
    "        \n",
    "        # Train ensemble classifier\n",
    "        df, et_classifier = train_ensemble_classifier(df, features)\n",
    "        \n",
    "        # Extract the optimal threshold from the classifier training output\n",
    "        # (This value is printed by train_ensemble_classifier function)\n",
    "        import re\n",
    "        from io import StringIO\n",
    "        import sys\n",
    "        \n",
    "        # Redirect stdout to capture it\n",
    "        old_stdout = sys.stdout\n",
    "        new_stdout = StringIO()\n",
    "        sys.stdout = new_stdout\n",
    "        \n",
    "        # Re-run the classification report to capture the threshold\n",
    "        if et_classifier is not None and hasattr(et_classifier, 'predict_proba'):\n",
    "            y_test = df['pseudo_label'].iloc[-int(len(df)*0.2):] # Approximate the test set\n",
    "            y_prob = et_classifier.predict_proba(df[features].iloc[-int(len(df)*0.2):])[:, 1]\n",
    "            from sklearn.metrics import precision_recall_curve\n",
    "            precision, recall, thresholds = precision_recall_curve(y_test, y_prob)\n",
    "            f1_scores = 2 * recall * precision / (recall + precision + 1e-10)\n",
    "            if np.all(np.isnan(f1_scores)):\n",
    "                optimal_threshold = 0.5\n",
    "            else:\n",
    "                optimal_idx = np.argmax(f1_scores)\n",
    "                if len(thresholds) > optimal_idx:\n",
    "                    optimal_threshold = thresholds[optimal_idx]\n",
    "                else:\n",
    "                    optimal_threshold = 0.5\n",
    "            print(f\"Optimal threshold: {optimal_threshold}\")\n",
    "        else:\n",
    "            optimal_threshold = 0.5\n",
    "        \n",
    "        # Restore stdout\n",
    "        sys.stdout = old_stdout\n",
    "        \n",
    "        # Extract the threshold from the captured output\n",
    "        output = new_stdout.getvalue()\n",
    "        threshold_match = re.search(r\"Optimal threshold: ([\\d.]+)\", output)\n",
    "        \n",
    "        if threshold_match:\n",
    "            optimal_threshold = float(threshold_match.group(1))\n",
    "        else:\n",
    "            # If we can't find it in the output, get it from the original output log\n",
    "            # assuming it's still in memory from the earlier run\n",
    "            threshold_match = re.search(r\"Optimal threshold: ([\\d.]+)\", \" \".join(sys.stdout.buffer))\n",
    "            if threshold_match:\n",
    "                optimal_threshold = float(threshold_match.group(1))\n",
    "            else:\n",
    "                # Fall back to default if we can't extract it\n",
    "                optimal_threshold = 0.5\n",
    "        \n",
    "        print(f\"Using optimal threshold: {optimal_threshold}\")\n",
    "        \n",
    "        # Aggregate results by author with the correct threshold\n",
    "        author_results = aggregate_results(df, optimal_threshold)\n",
    "        \n",
    "        # Visualize results - only if we have a classifier\n",
    "        if et_classifier is not None:\n",
    "            try:\n",
    "                visualize_results(df, author_results, et_classifier, features)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in visualization: {e}\")\n",
    "                # Continue even if visualization fails\n",
    "        \n",
    "        # Validate and save results with the correct threshold\n",
    "        validate_and_save_results(author_results, output_file, optimal_threshold)\n",
    "        \n",
    "        print(f\"Enhanced bot detection complete. Results saved to {output_file}\")\n",
    "        print(f\"Detected {author_results['is_bot'].sum()} bot accounts out of {len(author_results)} total accounts\")\n",
    "        \n",
    "        return author_results, df\n",
    "        \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"Error in bot detection pipeline: {e}\")\n",
    "        print(traceback.format_exc())\n",
    "        return None, merged_df\n",
    "   \n",
    "\n",
    "# Run the pipeline\n",
    "# Load the data\n",
    "df1 = pd.read_csv(\"Refugee_data_2023_bluesky_sentiment.csv\")\n",
    "df2 = pd.read_csv(\"Refugee_data_2024_bluesky_sentiment.csv\")\n",
    "# social_data_path = \"data.csv\"\n",
    "keywords_data_path = \"refugee_immigrant_keyword_list_v3.csv\"\n",
    "\n",
    "data_df = pd.concat([df1,df2])\n",
    "data_df = data_df.drop('Unnamed: 0', axis=1)\n",
    "keywords_df = pd.read_csv(keywords_data_path)\n",
    "keywords_df = keywords_df.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "# Merge posts data with keyword-subcategory mapping\n",
    "merged_df = data_df.merge(keywords_df, left_on=\"keyword\", right_on=\"term\", how=\"left\")\n",
    "# Convert 'created_at' column to datetime format\n",
    "merged_df['created_at'] = pd.to_datetime(merged_df['created_at'], format='mixed', utc=True)\n",
    "merged_df = merged_df[merged_df['created_at']<\"2025-01-01 18:18:04.312000+0000\"]\n",
    "# Add 'neutral' column: 1 if both happy and sad are 0, else 0\n",
    "merged_df['neutral'] = ((merged_df['happy'] == 0) & (merged_df['sad'] == 0)).astype(int)\n",
    "results, processed_df = bot_detection_pipeline(merged_df, 'bot_detection_results.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37fc9124-a539-4edd-81f3-cd1cd139c46e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU\n",
      "Starting GMAE2-CGNN Bot Detection\n",
      "Loading dataset...\n",
      "Loaded dataset with 1090909 posts and 304898 unique users\n",
      "[01:37:40] Running bot detection pipeline on full dataset...\n",
      "[01:37:40] Preprocessing full dataset...\n",
      "[01:37:40] Total qualified users with at least 2 posts: 126091\n",
      "[01:37:40] Processing batch 1/26\n",
      "[01:37:40] Processing 5000 new users in batch 1\n",
      "[01:37:40] Preprocessing data...\n",
      "[01:38:28] Preprocessing complete. 196157 posts from 5000 users\n",
      "[01:38:28] Extracting user features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5000/5000 [00:56<00:00, 88.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:39:25] Extracted 18 features for 5000 users\n",
      "[01:39:25] Constructing temporal interaction graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [02:52<00:00,  3.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:43:09] Graph constructed with 5000 nodes and 858799 edges\n",
      "[01:43:13] Training model...\n",
      "[01:43:14] Epoch 1/30, Loss: 0.697518\n",
      "[01:43:15] Epoch 5/30, Loss: 0.694738\n",
      "[01:43:16] Epoch 10/30, Loss: 0.693602\n",
      "[01:43:17] Epoch 15/30, Loss: 0.693301\n",
      "[01:43:19] Epoch 20/30, Loss: 0.693208\n",
      "[01:43:20] Epoch 25/30, Loss: 0.693170\n",
      "[01:43:21] Epoch 30/30, Loss: 0.693146\n",
      "[01:43:21] Training completed\n",
      "[01:43:21] Generating embeddings...\n",
      "[01:43:21] Detecting bots...\n",
      "[01:43:22] Detected 1907 bots out of 5000 users (38.1%)\n",
      "[01:43:22] Batch 1: Processed 5000 users, unique total so far: 5000\n",
      "[01:43:22] Processing batch 2/26\n",
      "[01:43:22] Processing 5000 new users in batch 2\n",
      "[01:43:22] Preprocessing data...\n",
      "[01:43:35] Preprocessing complete. 92569 posts from 5000 users\n",
      "[01:43:35] Extracting user features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5000/5000 [00:18<00:00, 272.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:43:53] Extracted 18 features for 5000 users\n",
      "[01:43:53] Constructing temporal interaction graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [02:51<00:00,  3.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:46:58] Graph constructed with 5000 nodes and 838526 edges\n",
      "[01:47:03] Training model...\n",
      "[01:47:03] Epoch 1/30, Loss: 0.697331\n",
      "[01:47:04] Epoch 5/30, Loss: 0.694993\n",
      "[01:47:05] Epoch 10/30, Loss: 0.693804\n",
      "[01:47:07] Epoch 15/30, Loss: 0.693480\n",
      "[01:47:08] Epoch 20/30, Loss: 0.693307\n",
      "[01:47:09] Epoch 25/30, Loss: 0.693190\n",
      "[01:47:11] Epoch 30/30, Loss: 0.693102\n",
      "[01:47:11] Training completed\n",
      "[01:47:11] Generating embeddings...\n",
      "[01:47:11] Detecting bots...\n",
      "[01:47:11] Detected 2485 bots out of 5000 users (49.7%)\n",
      "[01:47:11] Batch 2: Processed 5000 users, unique total so far: 10000\n",
      "[01:47:11] Processing batch 3/26\n",
      "[01:47:11] Processing 5000 new users in batch 3\n",
      "[01:47:11] Preprocessing data...\n",
      "[01:47:20] Preprocessing complete. 61834 posts from 5000 users\n",
      "[01:47:20] Extracting user features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5000/5000 [00:12<00:00, 410.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:47:32] Extracted 18 features for 5000 users\n",
      "[01:47:32] Constructing temporal interaction graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [02:51<00:00,  3.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:50:33] Graph constructed with 5000 nodes and 826132 edges\n",
      "[01:50:37] Training model...\n",
      "[01:50:38] Epoch 1/30, Loss: 0.693670\n",
      "[01:50:39] Epoch 5/30, Loss: 0.693118\n",
      "[01:50:40] Epoch 10/30, Loss: 0.692322\n",
      "[01:50:41] Epoch 15/30, Loss: 0.691666\n",
      "[01:50:43] Epoch 20/30, Loss: 0.691643\n",
      "[01:50:44] Epoch 25/30, Loss: 0.691646\n",
      "[01:50:45] Epoch 30/30, Loss: 0.691622\n",
      "[01:50:45] Training completed\n",
      "[01:50:45] Generating embeddings...\n",
      "[01:50:45] Detecting bots...\n",
      "[01:50:45] Detected 2708 bots out of 5000 users (54.2%)\n",
      "[01:50:45] Batch 3: Processed 5000 users, unique total so far: 15000\n",
      "[01:50:46] Processing batch 4/26\n",
      "[01:50:46] Processing 5000 new users in batch 4\n",
      "[01:50:46] Preprocessing data...\n",
      "[01:50:52] Preprocessing complete. 47186 posts from 5000 users\n",
      "[01:50:52] Extracting user features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5000/5000 [00:10<00:00, 481.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:51:03] Extracted 18 features for 5000 users\n",
      "[01:51:03] Constructing temporal interaction graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [02:25<00:00,  2.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:53:35] Graph constructed with 5000 nodes and 802948 edges\n",
      "[01:53:39] Training model...\n",
      "[01:53:40] Epoch 1/30, Loss: 0.693078\n",
      "[01:53:41] Epoch 5/30, Loss: 0.692036\n",
      "[01:53:42] Epoch 10/30, Loss: 0.691096\n",
      "[01:53:43] Epoch 15/30, Loss: 0.690954\n",
      "[01:53:45] Epoch 20/30, Loss: 0.690995\n",
      "[01:53:46] Epoch 25/30, Loss: 0.691014\n",
      "[01:53:47] Epoch 30/30, Loss: 0.691023\n",
      "[01:53:47] Training completed\n",
      "[01:53:47] Generating embeddings...\n",
      "[01:53:47] Detecting bots...\n",
      "[01:53:47] Detected 2164 bots out of 5000 users (43.3%)\n",
      "[01:53:47] Batch 4: Processed 5000 users, unique total so far: 20000\n",
      "[01:53:47] Processing batch 5/26\n",
      "[01:53:47] Processing 5000 new users in batch 5\n",
      "[01:53:47] Preprocessing data...\n",
      "[01:53:53] Preprocessing complete. 38016 posts from 5000 users\n",
      "[01:53:53] Extracting user features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5000/5000 [00:09<00:00, 550.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:54:02] Extracted 18 features for 5000 users\n",
      "[01:54:02] Constructing temporal interaction graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [01:35<00:00,  1.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:55:43] Graph constructed with 5000 nodes and 770855 edges\n",
      "[01:55:47] Training model...\n",
      "[01:55:48] Epoch 1/30, Loss: 0.693405\n",
      "[01:55:49] Epoch 5/30, Loss: 0.693002\n",
      "[01:55:50] Epoch 10/30, Loss: 0.692333\n",
      "[01:55:51] Epoch 15/30, Loss: 0.691308\n",
      "[01:55:52] Epoch 20/30, Loss: 0.690354\n",
      "[01:55:53] Epoch 25/30, Loss: 0.689868\n",
      "[01:55:55] Epoch 30/30, Loss: 0.689850\n",
      "[01:55:55] Training completed\n",
      "[01:55:55] Generating embeddings...\n",
      "[01:55:55] Detecting bots...\n",
      "[01:55:55] Detected 1923 bots out of 5000 users (38.5%)\n",
      "[01:55:55] Batch 5: Processed 5000 users, unique total so far: 25000\n",
      "[01:55:55] Processing batch 6/26\n",
      "[01:55:55] Processing 5000 new users in batch 6\n",
      "[01:55:55] Preprocessing data...\n",
      "[01:56:00] Preprocessing complete. 32281 posts from 5000 users\n",
      "[01:56:00] Extracting user features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5000/5000 [00:08<00:00, 620.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:56:08] Extracted 18 features for 5000 users\n",
      "[01:56:08] Constructing temporal interaction graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [01:09<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:57:22] Graph constructed with 5000 nodes and 731348 edges\n",
      "[01:57:26] Training model...\n",
      "[01:57:27] Epoch 1/30, Loss: 0.700591\n",
      "[01:57:28] Epoch 5/30, Loss: 0.694534\n",
      "[01:57:29] Epoch 10/30, Loss: 0.692867\n",
      "[01:57:30] Epoch 15/30, Loss: 0.691929\n",
      "[01:57:31] Epoch 20/30, Loss: 0.690633\n",
      "[01:57:32] Epoch 25/30, Loss: 0.689908\n",
      "[01:57:33] Epoch 30/30, Loss: 0.689212\n",
      "[01:57:33] Training completed\n",
      "[01:57:33] Generating embeddings...\n",
      "[01:57:33] Detecting bots...\n",
      "[01:57:33] Detected 1799 bots out of 5000 users (36.0%)\n",
      "[01:57:33] Batch 6: Processed 5000 users, unique total so far: 30000\n",
      "[01:57:34] Processing batch 7/26\n",
      "[01:57:34] Processing 5000 new users in batch 7\n",
      "[01:57:34] Preprocessing data...\n",
      "[01:57:38] Preprocessing complete. 28007 posts from 5000 users\n",
      "[01:57:38] Extracting user features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5000/5000 [00:07<00:00, 671.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:57:45] Extracted 18 features for 5000 users\n",
      "[01:57:45] Constructing temporal interaction graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:52<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:58:42] Graph constructed with 5000 nodes and 706007 edges\n",
      "[01:58:46] Training model...\n",
      "[01:58:46] Epoch 1/30, Loss: 0.694730\n",
      "[01:58:47] Epoch 5/30, Loss: 0.693023\n",
      "[01:58:48] Epoch 10/30, Loss: 0.691279\n",
      "[01:58:49] Epoch 15/30, Loss: 0.689756\n",
      "[01:58:51] Epoch 20/30, Loss: 0.688326\n",
      "[01:58:52] Epoch 25/30, Loss: 0.687435\n",
      "[01:58:53] Epoch 30/30, Loss: 0.687151\n",
      "[01:58:53] Training completed\n",
      "[01:58:53] Generating embeddings...\n",
      "[01:58:53] Detecting bots...\n",
      "[01:58:53] Detected 1662 bots out of 5000 users (33.2%)\n",
      "[01:58:53] Batch 7: Processed 5000 users, unique total so far: 35000\n",
      "[01:58:53] Processing batch 8/26\n",
      "[01:58:53] Processing 5000 new users in batch 8\n",
      "[01:58:53] Preprocessing data...\n",
      "[01:58:57] Preprocessing complete. 25000 posts from 5000 users\n",
      "[01:58:57] Extracting user features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5000/5000 [00:07<00:00, 671.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:59:05] Extracted 18 features for 5000 users\n",
      "[01:59:05] Constructing temporal interaction graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:41<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:59:50] Graph constructed with 5000 nodes and 647610 edges\n",
      "[01:59:54] Training model...\n",
      "[01:59:54] Epoch 1/30, Loss: 0.703104\n",
      "[01:59:55] Epoch 5/30, Loss: 0.696099\n",
      "[01:59:56] Epoch 10/30, Loss: 0.693721\n",
      "[01:59:57] Epoch 15/30, Loss: 0.692886\n",
      "[01:59:58] Epoch 20/30, Loss: 0.692045\n",
      "[01:59:59] Epoch 25/30, Loss: 0.690762\n",
      "[02:00:00] Epoch 30/30, Loss: 0.689105\n",
      "[02:00:00] Training completed\n",
      "[02:00:00] Generating embeddings...\n",
      "[02:00:00] Detecting bots...\n",
      "[02:00:01] Detected 1787 bots out of 5000 users (35.7%)\n",
      "[02:00:01] Batch 8: Processed 5000 users, unique total so far: 40000\n",
      "[02:00:01] Processing batch 9/26\n",
      "[02:00:01] Processing 5000 new users in batch 9\n",
      "[02:00:01] Preprocessing data...\n",
      "[02:00:04] Preprocessing complete. 21329 posts from 5000 users\n",
      "[02:00:04] Extracting user features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5000/5000 [00:06<00:00, 755.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:00:11] Extracted 18 features for 5000 users\n",
      "[02:00:11] Constructing temporal interaction graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:30<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:00:45] Graph constructed with 5000 nodes and 756714 edges\n",
      "[02:00:49] Training model...\n",
      "[02:00:50] Epoch 1/30, Loss: 0.694798\n",
      "[02:00:51] Epoch 5/30, Loss: 0.693202\n",
      "[02:00:52] Epoch 10/30, Loss: 0.692305\n",
      "[02:00:53] Epoch 15/30, Loss: 0.691356\n",
      "[02:00:54] Epoch 20/30, Loss: 0.690673\n",
      "[02:00:55] Epoch 25/30, Loss: 0.690419\n",
      "[02:00:57] Epoch 30/30, Loss: 0.690188\n",
      "[02:00:57] Training completed\n",
      "[02:00:57] Generating embeddings...\n",
      "[02:00:57] Detecting bots...\n",
      "[02:00:57] Detected 973 bots out of 5000 users (19.5%)\n",
      "[02:00:57] Batch 9: Processed 5000 users, unique total so far: 45000\n",
      "[02:00:57] Processing batch 10/26\n",
      "[02:00:57] Processing 5000 new users in batch 10\n",
      "[02:00:57] Preprocessing data...\n",
      "[02:01:00] Preprocessing complete. 20000 posts from 5000 users\n",
      "[02:01:00] Extracting user features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5000/5000 [00:06<00:00, 779.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:01:07] Extracted 18 features for 5000 users\n",
      "[02:01:07] Constructing temporal interaction graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:27<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:01:37] Graph constructed with 5000 nodes and 652418 edges\n",
      "[02:01:41] Training model...\n",
      "[02:01:41] Epoch 1/30, Loss: 0.693912\n",
      "[02:01:42] Epoch 5/30, Loss: 0.692223\n",
      "[02:01:43] Epoch 10/30, Loss: 0.689576\n",
      "[02:01:44] Epoch 15/30, Loss: 0.686939\n",
      "[02:01:45] Epoch 20/30, Loss: 0.685400\n",
      "[02:01:46] Epoch 25/30, Loss: 0.684724\n",
      "[02:01:47] Epoch 30/30, Loss: 0.684529\n",
      "[02:01:47] Training completed\n",
      "[02:01:47] Generating embeddings...\n",
      "[02:01:47] Detecting bots...\n",
      "[02:01:47] Detected 1472 bots out of 5000 users (29.4%)\n",
      "[02:01:47] Batch 10: Processed 5000 users, unique total so far: 50000\n",
      "[02:01:48] Processing batch 11/26\n",
      "[02:01:48] Processing 5000 new users in batch 11\n",
      "[02:01:48] Preprocessing data...\n",
      "[02:01:51] Preprocessing complete. 19296 posts from 5000 users\n",
      "[02:01:51] Extracting user features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5000/5000 [00:06<00:00, 784.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:01:57] Extracted 18 features for 5000 users\n",
      "[02:01:57] Constructing temporal interaction graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:25<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:02:26] Graph constructed with 5000 nodes and 475154 edges\n",
      "[02:02:29] Training model...\n",
      "[02:02:29] Epoch 1/30, Loss: 0.693485\n",
      "[02:02:30] Epoch 5/30, Loss: 0.690705\n",
      "[02:02:30] Epoch 10/30, Loss: 0.686078\n",
      "[02:02:31] Epoch 15/30, Loss: 0.681849\n",
      "[02:02:32] Epoch 20/30, Loss: 0.678362\n",
      "[02:02:33] Epoch 25/30, Loss: 0.677495\n",
      "[02:02:34] Epoch 30/30, Loss: 0.677236\n",
      "[02:02:34] Training completed\n",
      "[02:02:34] Generating embeddings...\n",
      "[02:02:34] Detecting bots...\n",
      "[02:02:34] Detected 2396 bots out of 5000 users (47.9%)\n",
      "[02:02:34] Batch 11: Processed 5000 users, unique total so far: 55000\n",
      "[02:02:34] Processing batch 12/26\n",
      "[02:02:34] Processing 5000 new users in batch 12\n",
      "[02:02:34] Preprocessing data...\n",
      "[02:02:37] Preprocessing complete. 15000 posts from 5000 users\n",
      "[02:02:37] Extracting user features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5000/5000 [00:06<00:00, 829.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:02:43] Extracted 18 features for 5000 users\n",
      "[02:02:43] Constructing temporal interaction graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:15<00:00,  3.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:03:01] Graph constructed with 5000 nodes and 556813 edges\n",
      "[02:03:04] Training model...\n",
      "[02:03:04] Epoch 1/30, Loss: 0.701391\n",
      "[02:03:05] Epoch 5/30, Loss: 0.694707\n",
      "[02:03:06] Epoch 10/30, Loss: 0.692489\n",
      "[02:03:07] Epoch 15/30, Loss: 0.691225\n",
      "[02:03:08] Epoch 20/30, Loss: 0.689853\n",
      "[02:03:09] Epoch 25/30, Loss: 0.688052\n",
      "[02:03:10] Epoch 30/30, Loss: 0.686204\n",
      "[02:03:10] Training completed\n",
      "[02:03:10] Generating embeddings...\n",
      "[02:03:10] Detecting bots...\n",
      "[02:03:10] Detected 1150 bots out of 5000 users (23.0%)\n",
      "[02:03:10] Batch 12: Processed 5000 users, unique total so far: 60000\n",
      "[02:03:10] Processing batch 13/26\n",
      "[02:03:10] Processing 5000 new users in batch 13\n",
      "[02:03:10] Preprocessing data...\n",
      "[02:03:13] Preprocessing complete. 15000 posts from 5000 users\n",
      "[02:03:13] Extracting user features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5000/5000 [00:06<00:00, 830.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:03:19] Extracted 18 features for 5000 users\n",
      "[02:03:19] Constructing temporal interaction graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:15<00:00,  3.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:03:37] Graph constructed with 5000 nodes and 488142 edges\n",
      "[02:03:40] Training model...\n",
      "[02:03:40] Epoch 1/30, Loss: 0.691004\n",
      "[02:03:41] Epoch 5/30, Loss: 0.686256\n",
      "[02:03:42] Epoch 10/30, Loss: 0.680266\n",
      "[02:03:43] Epoch 15/30, Loss: 0.676246\n",
      "[02:03:44] Epoch 20/30, Loss: 0.673660\n",
      "[02:03:44] Epoch 25/30, Loss: 0.672517\n",
      "[02:03:45] Epoch 30/30, Loss: 0.671655\n",
      "[02:03:45] Training completed\n",
      "[02:03:45] Generating embeddings...\n",
      "[02:03:45] Detecting bots...\n",
      "[02:03:45] Detected 1759 bots out of 5000 users (35.2%)\n",
      "[02:03:45] Batch 13: Processed 5000 users, unique total so far: 65000\n",
      "[02:03:46] Processing batch 14/26\n",
      "[02:03:46] Processing 5000 new users in batch 14\n",
      "[02:03:46] Preprocessing data...\n",
      "[02:03:48] Preprocessing complete. 15000 posts from 5000 users\n",
      "[02:03:48] Extracting user features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5000/5000 [00:06<00:00, 831.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:03:54] Extracted 18 features for 5000 users\n",
      "[02:03:54] Constructing temporal interaction graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:15<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:04:13] Graph constructed with 5000 nodes and 520920 edges\n",
      "[02:04:16] Training model...\n",
      "[02:04:16] Epoch 1/30, Loss: 0.695158\n",
      "[02:04:17] Epoch 5/30, Loss: 0.693054\n",
      "[02:04:18] Epoch 10/30, Loss: 0.690936\n",
      "[02:04:19] Epoch 15/30, Loss: 0.687687\n",
      "[02:04:20] Epoch 20/30, Loss: 0.683646\n",
      "[02:04:20] Epoch 25/30, Loss: 0.679115\n",
      "[02:04:21] Epoch 30/30, Loss: 0.676221\n",
      "[02:04:21] Training completed\n",
      "[02:04:21] Generating embeddings...\n",
      "[02:04:21] Detecting bots...\n",
      "[02:04:21] Detected 1383 bots out of 5000 users (27.7%)\n",
      "[02:04:21] Batch 14: Processed 5000 users, unique total so far: 70000\n",
      "[02:04:22] Processing batch 15/26\n",
      "[02:04:22] Processing 5000 new users in batch 15\n",
      "[02:04:22] Preprocessing data...\n",
      "[02:04:24] Preprocessing complete. 15000 posts from 5000 users\n",
      "[02:04:24] Extracting user features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5000/5000 [00:06<00:00, 829.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:04:30] Extracted 18 features for 5000 users\n",
      "[02:04:30] Constructing temporal interaction graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [11:14<00:00, 13.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:15:48] Graph constructed with 5000 nodes and 562108 edges\n",
      "[02:15:51] Training model...\n",
      "[02:15:51] Epoch 1/30, Loss: 0.693530\n",
      "[02:15:52] Epoch 5/30, Loss: 0.692174\n",
      "[02:15:53] Epoch 10/30, Loss: 0.690168\n",
      "[02:15:54] Epoch 15/30, Loss: 0.687780\n",
      "[02:15:55] Epoch 20/30, Loss: 0.685294\n",
      "[02:15:56] Epoch 25/30, Loss: 0.683066\n",
      "[02:15:57] Epoch 30/30, Loss: 0.681695\n",
      "[02:15:57] Training completed\n",
      "[02:15:57] Generating embeddings...\n",
      "[02:15:57] Detecting bots...\n",
      "[02:15:57] Detected 1031 bots out of 5000 users (20.6%)\n",
      "[02:15:57] Batch 15: Processed 5000 users, unique total so far: 75000\n",
      "[02:15:58] Processing batch 16/26\n",
      "[02:15:58] Processing 5000 new users in batch 16\n",
      "[02:15:58] Preprocessing data...\n",
      "[02:16:00] Preprocessing complete. 11706 posts from 5000 users\n",
      "[02:16:00] Extracting user features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5000/5000 [00:05<00:00, 948.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:16:05] Extracted 18 features for 5000 users\n",
      "[02:16:05] Constructing temporal interaction graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:09<00:00,  5.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:16:18] Graph constructed with 5000 nodes and 608153 edges\n",
      "[02:16:21] Training model...\n",
      "[02:16:21] Epoch 1/30, Loss: 0.693189\n",
      "[02:16:22] Epoch 5/30, Loss: 0.692580\n",
      "[02:16:23] Epoch 10/30, Loss: 0.690958\n",
      "[02:16:24] Epoch 15/30, Loss: 0.689513\n",
      "[02:16:25] Epoch 20/30, Loss: 0.688434\n",
      "[02:16:26] Epoch 25/30, Loss: 0.687555\n",
      "[02:16:27] Epoch 30/30, Loss: 0.686878\n",
      "[02:16:27] Training completed\n",
      "[02:16:27] Generating embeddings...\n",
      "[02:16:27] Detecting bots...\n",
      "[02:16:27] Detected 940 bots out of 5000 users (18.8%)\n",
      "[02:16:27] Batch 16: Processed 5000 users, unique total so far: 80000\n",
      "[02:16:27] Processing batch 17/26\n",
      "[02:16:27] Processing 5000 new users in batch 17\n",
      "[02:16:27] Preprocessing data...\n",
      "[02:16:30] Preprocessing complete. 10000 posts from 5000 users\n",
      "[02:16:30] Extracting user features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5000/5000 [00:05<00:00, 967.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:16:35] Extracted 18 features for 5000 users\n",
      "[02:16:35] Constructing temporal interaction graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:07<00:00,  6.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:16:44] Graph constructed with 5000 nodes and 510165 edges\n",
      "[02:16:47] Training model...\n",
      "[02:16:47] Epoch 1/30, Loss: 0.692846\n",
      "[02:16:48] Epoch 5/30, Loss: 0.691405\n",
      "[02:16:49] Epoch 10/30, Loss: 0.689136\n",
      "[02:16:50] Epoch 15/30, Loss: 0.687204\n",
      "[02:16:51] Epoch 20/30, Loss: 0.685279\n",
      "[02:16:52] Epoch 25/30, Loss: 0.683934\n",
      "[02:16:52] Epoch 30/30, Loss: 0.682944\n",
      "[02:16:52] Training completed\n",
      "[02:16:52] Generating embeddings...\n",
      "[02:16:52] Detecting bots...\n",
      "[02:16:53] Detected 962 bots out of 5000 users (19.2%)\n",
      "[02:16:53] Batch 17: Processed 5000 users, unique total so far: 85000\n",
      "[02:16:53] Processing batch 18/26\n",
      "[02:16:53] Processing 5000 new users in batch 18\n",
      "[02:16:53] Preprocessing data...\n",
      "[02:16:55] Preprocessing complete. 10000 posts from 5000 users\n",
      "[02:16:55] Extracting user features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5000/5000 [00:05<00:00, 943.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:17:00] Extracted 18 features for 5000 users\n",
      "[02:17:00] Constructing temporal interaction graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:07<00:00,  6.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:17:10] Graph constructed with 5000 nodes and 458034 edges\n",
      "[02:17:12] Training model...\n",
      "[02:17:12] Epoch 1/30, Loss: 0.694057\n",
      "[02:17:13] Epoch 5/30, Loss: 0.692598\n",
      "[02:17:14] Epoch 10/30, Loss: 0.691868\n",
      "[02:17:15] Epoch 15/30, Loss: 0.690435\n",
      "[02:17:16] Epoch 20/30, Loss: 0.688658\n",
      "[02:17:16] Epoch 25/30, Loss: 0.686661\n",
      "[02:17:17] Epoch 30/30, Loss: 0.684841\n",
      "[02:17:17] Training completed\n",
      "[02:17:17] Generating embeddings...\n",
      "[02:17:17] Detecting bots...\n",
      "[02:17:17] Detected 1002 bots out of 5000 users (20.0%)\n",
      "[02:17:17] Batch 18: Processed 5000 users, unique total so far: 90000\n",
      "[02:17:18] Processing batch 19/26\n",
      "[02:17:18] Processing 5000 new users in batch 19\n",
      "[02:17:18] Preprocessing data...\n",
      "[02:17:20] Preprocessing complete. 10000 posts from 5000 users\n",
      "[02:17:20] Extracting user features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5000/5000 [00:05<00:00, 956.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:17:25] Extracted 18 features for 5000 users\n",
      "[02:17:25] Constructing temporal interaction graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:07<00:00,  6.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:17:35] Graph constructed with 5000 nodes and 419209 edges\n",
      "[02:17:37] Training model...\n",
      "[02:17:37] Epoch 1/30, Loss: 0.700030\n",
      "[02:17:38] Epoch 5/30, Loss: 0.693391\n",
      "[02:17:38] Epoch 10/30, Loss: 0.690883\n",
      "[02:17:39] Epoch 15/30, Loss: 0.689312\n",
      "[02:17:40] Epoch 20/30, Loss: 0.687646\n",
      "[02:17:40] Epoch 25/30, Loss: 0.685414\n",
      "[02:17:41] Epoch 30/30, Loss: 0.683680\n",
      "[02:17:41] Training completed\n",
      "[02:17:41] Generating embeddings...\n",
      "[02:17:41] Detecting bots...\n",
      "[02:17:41] Detected 1184 bots out of 5000 users (23.7%)\n",
      "[02:17:41] Batch 19: Processed 5000 users, unique total so far: 95000\n",
      "[02:17:41] Processing batch 20/26\n",
      "[02:17:41] Processing 5000 new users in batch 20\n",
      "[02:17:41] Preprocessing data...\n",
      "[02:17:43] Preprocessing complete. 10000 posts from 5000 users\n",
      "[02:17:43] Extracting user features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5000/5000 [00:05<00:00, 959.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:17:49] Extracted 18 features for 5000 users\n",
      "[02:17:49] Constructing temporal interaction graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:07<00:00,  6.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:17:58] Graph constructed with 5000 nodes and 426944 edges\n",
      "[02:18:01] Training model...\n",
      "[02:18:01] Epoch 1/30, Loss: 0.694317\n",
      "[02:18:02] Epoch 5/30, Loss: 0.691050\n",
      "[02:18:03] Epoch 10/30, Loss: 0.684236\n",
      "[02:18:03] Epoch 15/30, Loss: 0.679889\n",
      "[02:18:04] Epoch 20/30, Loss: 0.675410\n",
      "[02:18:05] Epoch 25/30, Loss: 0.672793\n",
      "[02:18:06] Epoch 30/30, Loss: 0.671762\n",
      "[02:18:06] Training completed\n",
      "[02:18:06] Generating embeddings...\n",
      "[02:18:06] Detecting bots...\n",
      "[02:18:06] Detected 2319 bots out of 5000 users (46.4%)\n",
      "[02:18:06] Batch 20: Processed 5000 users, unique total so far: 100000\n",
      "[02:18:06] Processing batch 21/26\n",
      "[02:18:06] Processing 5000 new users in batch 21\n",
      "[02:18:06] Preprocessing data...\n",
      "[02:18:08] Preprocessing complete. 10000 posts from 5000 users\n",
      "[02:18:08] Extracting user features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5000/5000 [00:05<00:00, 934.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:18:14] Extracted 18 features for 5000 users\n",
      "[02:18:14] Constructing temporal interaction graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:07<00:00,  6.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:18:23] Graph constructed with 5000 nodes and 543676 edges\n",
      "[02:18:26] Training model...\n",
      "[02:18:27] Epoch 1/30, Loss: 0.693387\n",
      "[02:18:27] Epoch 5/30, Loss: 0.692203\n",
      "[02:18:28] Epoch 10/30, Loss: 0.690452\n",
      "[02:18:29] Epoch 15/30, Loss: 0.688615\n",
      "[02:18:30] Epoch 20/30, Loss: 0.686639\n",
      "[02:18:31] Epoch 25/30, Loss: 0.685339\n",
      "[02:18:32] Epoch 30/30, Loss: 0.684431\n",
      "[02:18:32] Training completed\n",
      "[02:18:32] Generating embeddings...\n",
      "[02:18:32] Detecting bots...\n",
      "[02:18:32] Detected 1730 bots out of 5000 users (34.6%)\n",
      "[02:18:32] Batch 21: Processed 5000 users, unique total so far: 105000\n",
      "[02:18:32] Processing batch 22/26\n",
      "[02:18:32] Processing 5000 new users in batch 22\n",
      "[02:18:32] Preprocessing data...\n",
      "[02:18:35] Preprocessing complete. 10000 posts from 5000 users\n",
      "[02:18:35] Extracting user features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5000/5000 [00:05<00:00, 934.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:18:40] Extracted 18 features for 5000 users\n",
      "[02:18:40] Constructing temporal interaction graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:07<00:00,  6.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:18:50] Graph constructed with 5000 nodes and 319994 edges\n",
      "[02:18:51] Training model...\n",
      "[02:18:51] Epoch 1/30, Loss: 0.695150\n",
      "[02:18:52] Epoch 5/30, Loss: 0.690077\n",
      "[02:18:52] Epoch 10/30, Loss: 0.686268\n",
      "[02:18:53] Epoch 15/30, Loss: 0.681608\n",
      "[02:18:54] Epoch 20/30, Loss: 0.676513\n",
      "[02:18:54] Epoch 25/30, Loss: 0.670770\n",
      "[02:18:55] Epoch 30/30, Loss: 0.665677\n",
      "[02:18:55] Training completed\n",
      "[02:18:55] Generating embeddings...\n",
      "[02:18:55] Detecting bots...\n",
      "[02:18:55] Detected 1832 bots out of 5000 users (36.6%)\n",
      "[02:18:55] Batch 22: Processed 5000 users, unique total so far: 110000\n",
      "[02:18:55] Processing batch 23/26\n",
      "[02:18:55] Processing 5000 new users in batch 23\n",
      "[02:18:55] Preprocessing data...\n",
      "[02:18:57] Preprocessing complete. 10000 posts from 5000 users\n",
      "[02:18:57] Extracting user features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5000/5000 [00:05<00:00, 915.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:19:03] Extracted 18 features for 5000 users\n",
      "[02:19:03] Constructing temporal interaction graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:07<00:00,  6.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:19:12] Graph constructed with 5000 nodes and 273552 edges\n",
      "[02:19:14] Training model...\n",
      "[02:19:14] Epoch 1/30, Loss: 0.693134\n",
      "[02:19:14] Epoch 5/30, Loss: 0.686800\n",
      "[02:19:15] Epoch 10/30, Loss: 0.674761\n",
      "[02:19:15] Epoch 15/30, Loss: 0.660888\n",
      "[02:19:16] Epoch 20/30, Loss: 0.651011\n",
      "[02:19:16] Epoch 25/30, Loss: 0.645604\n",
      "[02:19:17] Epoch 30/30, Loss: 0.642327\n",
      "[02:19:17] Training completed\n",
      "[02:19:17] Generating embeddings...\n",
      "[02:19:17] Detecting bots...\n",
      "[02:19:17] Detected 2589 bots out of 5000 users (51.8%)\n",
      "[02:19:17] Batch 23: Processed 5000 users, unique total so far: 115000\n",
      "[02:19:17] Processing batch 24/26\n",
      "[02:19:17] Processing 5000 new users in batch 24\n",
      "[02:19:17] Preprocessing data...\n",
      "[02:19:19] Preprocessing complete. 10000 posts from 5000 users\n",
      "[02:19:19] Extracting user features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5000/5000 [00:05<00:00, 943.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:19:24] Extracted 18 features for 5000 users\n",
      "[02:19:24] Constructing temporal interaction graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:07<00:00,  6.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:19:34] Graph constructed with 5000 nodes and 398192 edges\n",
      "[02:19:36] Training model...\n",
      "[02:19:37] Epoch 1/30, Loss: 0.693136\n",
      "[02:19:37] Epoch 5/30, Loss: 0.691197\n",
      "[02:19:38] Epoch 10/30, Loss: 0.687516\n",
      "[02:19:38] Epoch 15/30, Loss: 0.682672\n",
      "[02:19:39] Epoch 20/30, Loss: 0.677693\n",
      "[02:19:40] Epoch 25/30, Loss: 0.672743\n",
      "[02:19:40] Epoch 30/30, Loss: 0.668990\n",
      "[02:19:40] Training completed\n",
      "[02:19:40] Generating embeddings...\n",
      "[02:19:40] Detecting bots...\n",
      "[02:19:41] Detected 1460 bots out of 5000 users (29.2%)\n",
      "[02:19:41] Batch 24: Processed 5000 users, unique total so far: 120000\n",
      "[02:19:41] Processing batch 25/26\n",
      "[02:19:41] Processing 5000 new users in batch 25\n",
      "[02:19:41] Preprocessing data...\n",
      "[02:19:43] Preprocessing complete. 10000 posts from 5000 users\n",
      "[02:19:43] Extracting user features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5000/5000 [00:05<00:00, 943.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:19:48] Extracted 18 features for 5000 users\n",
      "[02:19:48] Constructing temporal interaction graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [00:07<00:00,  6.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:19:58] Graph constructed with 5000 nodes and 446477 edges\n",
      "[02:20:00] Training model...\n",
      "[02:20:01] Epoch 1/30, Loss: 0.694775\n",
      "[02:20:01] Epoch 5/30, Loss: 0.691670\n",
      "[02:20:02] Epoch 10/30, Loss: 0.688658\n",
      "[02:20:03] Epoch 15/30, Loss: 0.683930\n",
      "[02:20:04] Epoch 20/30, Loss: 0.678539\n",
      "[02:20:05] Epoch 25/30, Loss: 0.674032\n",
      "[02:20:05] Epoch 30/30, Loss: 0.670983\n",
      "[02:20:05] Training completed\n",
      "[02:20:05] Generating embeddings...\n",
      "[02:20:05] Detecting bots...\n",
      "[02:20:06] Detected 1315 bots out of 5000 users (26.3%)\n",
      "[02:20:06] Batch 25: Processed 5000 users, unique total so far: 125000\n",
      "[02:20:06] Processing batch 26/26\n",
      "[02:20:06] Processing 1091 new users in batch 26\n",
      "[02:20:06] Preprocessing data...\n",
      "[02:20:06] Preprocessing complete. 2182 posts from 1091 users\n",
      "[02:20:06] Extracting user features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1091/1091 [00:00<00:00, 1141.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:20:07] Extracted 18 features for 1091 users\n",
      "[02:20:07] Constructing temporal interaction graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 11/11 [00:01<00:00,  6.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:20:09] Graph constructed with 1091 nodes and 98823 edges\n",
      "[02:20:09] Training model...\n",
      "[02:20:09] Epoch 1/30, Loss: 0.694547\n",
      "[02:20:09] Epoch 5/30, Loss: 0.692142\n",
      "[02:20:10] Epoch 10/30, Loss: 0.687809\n",
      "[02:20:10] Epoch 15/30, Loss: 0.683881\n",
      "[02:20:10] Epoch 20/30, Loss: 0.678636\n",
      "[02:20:10] Epoch 25/30, Loss: 0.674679\n",
      "[02:20:10] Epoch 30/30, Loss: 0.672915\n",
      "[02:20:10] Training completed\n",
      "[02:20:10] Generating embeddings...\n",
      "[02:20:10] Detecting bots...\n",
      "[02:20:10] Detected 270 bots out of 1091 users (24.7%)\n",
      "[02:20:10] Batch 26: Processed 1091 users, unique total so far: 126091\n",
      "[02:20:11] Final results: Detected 42202 bots out of 126091 unique users (33.5%)\n",
      "[02:20:11] Final results saved to gmae2_cgnn_results/full_dataset_results.csv\n",
      "Detected 42202 bots out of 126091 users (33.5%)\n",
      "Results saved to gmae2_cgnn_results/\n"
     ]
    }
   ],
   "source": [
    "#Code 2:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "import random\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from datetime import datetime, timedelta\n",
    "from scipy import sparse\n",
    "import gc\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "# Disable parallelism warnings\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Set random seeds\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class FeatureEncoder(nn.Module):\n",
    "    \"\"\"Neural network for encoding user features\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dims=[128, 64, 32]):\n",
    "        super(FeatureEncoder, self).__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, dim))\n",
    "            layers.append(nn.BatchNorm1d(dim))\n",
    "            layers.append(nn.LeakyReLU())\n",
    "            layers.append(nn.Dropout(0.2))\n",
    "            prev_dim = dim\n",
    "            \n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class GraphEncoder(nn.Module):\n",
    "    \"\"\"Simple graph encoder using GNN principles\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim=32, output_dim=16):\n",
    "        super(GraphEncoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x, adj_matrix):\n",
    "        # Simple graph convolution: x' = Ax\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = torch.mm(adj_matrix, x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "class BotDetector:\n",
    "    \"\"\"\n",
    "    GMAE2-CGNN Bot Detector - Optimized for Research\n",
    "    Based on \"Unsupervised Social Bot Detection via Structural Information Theory\" (2024)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, verbose=True, output_dir=\"bot_detection_results\"):\n",
    "        self.verbose = verbose\n",
    "        self.output_dir = output_dir\n",
    "        \n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Initialize components\n",
    "        self.feature_encoder = None\n",
    "        self.graph_encoder = None\n",
    "        self.embedding_model = None\n",
    "        self.user_features = None\n",
    "        self.feature_matrix = None\n",
    "        self.graph = None\n",
    "        self.adj_matrix = None\n",
    "        self.user_to_node = {}\n",
    "        self.node_to_user = {}\n",
    "        self.user_embeddings = None\n",
    "        self.bot_scores = None\n",
    "        self.unique_users = None\n",
    "        \n",
    "        # Statistics tracking\n",
    "        self.run_stats = {\n",
    "            'start_time': None,\n",
    "            'end_time': None,\n",
    "            'total_users': 0,\n",
    "            'processed_users': 0,\n",
    "            'detected_bots': 0,\n",
    "            'bot_percentage': 0.0,\n",
    "            'batches': []\n",
    "        }\n",
    "        \n",
    "    def log(self, message):\n",
    "        \"\"\"Print log message if verbose is enabled\"\"\"\n",
    "        if self.verbose:\n",
    "            timestamp = datetime.now().strftime('%H:%M:%S')\n",
    "            print(f\"[{timestamp}] {message}\")\n",
    "    \n",
    "    def preprocess_data(self, df, max_users=10000, min_posts=2, max_posts=50):\n",
    "        \"\"\"Preprocess the data\"\"\"\n",
    "        self.log(\"Preprocessing data...\")\n",
    "        \n",
    "        # Ensure created_at is datetime\n",
    "        if not pd.api.types.is_datetime64_dtype(df['created_at']):\n",
    "            df['created_at'] = pd.to_datetime(df['created_at'], format='mixed', utc=True)\n",
    "        \n",
    "        # Add neutral sentiment if not present\n",
    "        if 'neutral' not in df.columns:\n",
    "            df['neutral'] = ((df['happy'] == 0) & (df['sad'] == 0)).astype(int)\n",
    "        \n",
    "        # Filter users with enough posts\n",
    "        user_counts = df['author_handle'].value_counts()\n",
    "        qualified_users = user_counts[user_counts >= min_posts].index\n",
    "        \n",
    "        # Sample users if too many\n",
    "        if len(qualified_users) > max_users:\n",
    "            self.log(f\"Sampling {max_users} users from {len(qualified_users)} qualified users\")\n",
    "            qualified_users = np.random.choice(qualified_users, max_users, replace=False)\n",
    "        \n",
    "        # Filter data to qualified users\n",
    "        filtered_df = df[df['author_handle'].isin(qualified_users)].copy()\n",
    "        \n",
    "        # Sample posts for very active users\n",
    "        if max_posts:\n",
    "            user_samples = []\n",
    "            for user in qualified_users:\n",
    "                user_posts = filtered_df[filtered_df['author_handle'] == user]\n",
    "                if len(user_posts) > max_posts:\n",
    "                    user_posts = user_posts.sample(n=max_posts, random_state=SEED)\n",
    "                user_samples.append(user_posts)\n",
    "            filtered_df = pd.concat(user_samples)\n",
    "        \n",
    "        # Create user-node mappings\n",
    "        unique_users = filtered_df['author_handle'].unique()\n",
    "        self.user_to_node = {user: i for i, user in enumerate(unique_users)}\n",
    "        self.node_to_user = {i: user for user, i in self.user_to_node.items()}\n",
    "        self.unique_users = unique_users\n",
    "        \n",
    "        self.log(f\"Preprocessing complete. {len(filtered_df)} posts from {len(unique_users)} users\")\n",
    "        return filtered_df\n",
    "    \n",
    "    def extract_features(self, df):\n",
    "        \"\"\"Extract user features with text embedding disabled for efficiency\"\"\"\n",
    "        self.log(\"Extracting user features...\")\n",
    "        \n",
    "        # Get unique users\n",
    "        unique_users = list(self.user_to_node.keys())\n",
    "        \n",
    "        user_features = []\n",
    "        for user in tqdm(unique_users, disable=not self.verbose):\n",
    "            user_posts = df[df['author_handle'] == user]\n",
    "            \n",
    "            # Basic user metrics\n",
    "            post_count = len(user_posts)\n",
    "            \n",
    "            # Temporal features\n",
    "            post_times = pd.to_datetime(user_posts['created_at'])\n",
    "            if post_count > 1:\n",
    "                sorted_times = sorted(post_times)\n",
    "                time_diffs = [(sorted_times[i] - sorted_times[i-1]).total_seconds() / 60 \n",
    "                              for i in range(1, len(sorted_times))]\n",
    "                \n",
    "                avg_time_diff = np.mean(time_diffs)\n",
    "                std_time_diff = np.std(time_diffs) if len(time_diffs) > 1 else 0\n",
    "                \n",
    "                # Hour distribution\n",
    "                hours = post_times.dt.hour\n",
    "                hour_counts = hours.value_counts(normalize=True)\n",
    "                hour_entropy = -sum(p * np.log2(p) for p in hour_counts if p > 0)\n",
    "                \n",
    "                # Day of week pattern\n",
    "                days = post_times.dt.dayofweek\n",
    "                day_counts = days.value_counts(normalize=True)\n",
    "                day_entropy = -sum(p * np.log2(p) for p in day_counts if p > 0)\n",
    "            else:\n",
    "                avg_time_diff = std_time_diff = hour_entropy = day_entropy = 0\n",
    "            \n",
    "            # Engagement metrics\n",
    "            avg_replies = user_posts['reply_count'].mean()\n",
    "            avg_reposts = user_posts['repost_count'].mean()\n",
    "            avg_likes = user_posts['like_count'].mean()\n",
    "            \n",
    "            # Engagement ratios\n",
    "            if post_count > 0:\n",
    "                reply_ratio = user_posts['reply_count'].sum() / post_count\n",
    "                repost_ratio = user_posts['repost_count'].sum() / post_count\n",
    "                like_ratio = user_posts['like_count'].sum() / post_count\n",
    "            else:\n",
    "                reply_ratio = repost_ratio = like_ratio = 0\n",
    "            \n",
    "            # Sentiment metrics\n",
    "            happy_ratio = user_posts['happy'].mean()\n",
    "            sad_ratio = user_posts['sad'].mean()\n",
    "            neutral_ratio = user_posts['neutral'].mean()\n",
    "            \n",
    "            # Sentiment entropy\n",
    "            sentiment_counts = np.array([\n",
    "                np.sum(user_posts['happy'] > 0),\n",
    "                np.sum(user_posts['sad'] > 0),\n",
    "                np.sum(user_posts['neutral'] > 0)\n",
    "            ])\n",
    "            sentiment_dist = sentiment_counts / np.sum(sentiment_counts) if np.sum(sentiment_counts) > 0 else np.ones(3)/3\n",
    "            sentiment_entropy = -np.sum(sentiment_dist * np.log2(sentiment_dist + 1e-10))\n",
    "            \n",
    "            # Text metrics (using post_text instead of cleaned_text)\n",
    "            if 'post_text' in user_posts.columns:\n",
    "                text_lengths = user_posts['post_text'].astype(str).apply(len)\n",
    "                avg_text_length = text_lengths.mean()\n",
    "                std_text_length = text_lengths.std() if len(text_lengths) > 1 else 0\n",
    "                cv_text_length = std_text_length / avg_text_length if avg_text_length > 0 else 0\n",
    "            else:\n",
    "                avg_text_length = std_text_length = cv_text_length = 0\n",
    "            \n",
    "            # Assemble features\n",
    "            features = {\n",
    "                'user': user,\n",
    "                'post_count': post_count,\n",
    "                'avg_time_diff': avg_time_diff,\n",
    "                'std_time_diff': std_time_diff,\n",
    "                'hour_entropy': hour_entropy,\n",
    "                'day_entropy': day_entropy,\n",
    "                'avg_replies': avg_replies,\n",
    "                'avg_reposts': avg_reposts,\n",
    "                'avg_likes': avg_likes,\n",
    "                'reply_ratio': reply_ratio,\n",
    "                'repost_ratio': repost_ratio,\n",
    "                'like_ratio': like_ratio,\n",
    "                'happy_ratio': happy_ratio,\n",
    "                'sad_ratio': sad_ratio,\n",
    "                'neutral_ratio': neutral_ratio,\n",
    "                'sentiment_entropy': sentiment_entropy,\n",
    "                'avg_text_length': avg_text_length,\n",
    "                'std_text_length': std_text_length,\n",
    "                'cv_text_length': cv_text_length\n",
    "            }\n",
    "            \n",
    "            user_features.append(features)\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        user_df = pd.DataFrame(user_features)\n",
    "        self.user_features = user_df\n",
    "        \n",
    "        # Create feature matrix\n",
    "        feature_cols = [col for col in user_df.columns if col != 'user']\n",
    "        scaler = StandardScaler()\n",
    "        self.feature_matrix = scaler.fit_transform(user_df[feature_cols])\n",
    "        \n",
    "        self.log(f\"Extracted {len(feature_cols)} features for {len(user_df)} users\")\n",
    "        return user_df\n",
    "    \n",
    "    def construct_graph(self, df, temporal_window_days=14):\n",
    "        \"\"\"Construct graph using non-parallel approach\"\"\"\n",
    "        self.log(\"Constructing temporal interaction graph...\")\n",
    "        \n",
    "        # Create graph\n",
    "        G = nx.Graph()\n",
    "        for user, node_id in self.user_to_node.items():\n",
    "            G.add_node(node_id, user=user)\n",
    "        \n",
    "        # Simplified approach to avoid parallelism deadlocks\n",
    "        temporal_window = timedelta(days=temporal_window_days)\n",
    "        \n",
    "        # Get post times by user (non-parallel)\n",
    "        user_post_times = {}\n",
    "        for user in self.user_to_node.keys():\n",
    "            user_posts = df[df['author_handle'] == user]\n",
    "            user_post_times[user] = pd.to_datetime(user_posts['created_at']).tolist()\n",
    "        \n",
    "        # Create edges based on temporal patterns (simplified)\n",
    "        edges = []\n",
    "        users = list(self.user_to_node.keys())\n",
    "        \n",
    "        # Process in chunks to manage memory\n",
    "        chunk_size = 100\n",
    "        num_chunks = (len(users) + chunk_size - 1) // chunk_size\n",
    "        \n",
    "        for chunk_idx in tqdm(range(num_chunks), disable=not self.verbose):\n",
    "            start_idx = chunk_idx * chunk_size\n",
    "            end_idx = min((chunk_idx + 1) * chunk_size, len(users))\n",
    "            user_chunk = users[start_idx:end_idx]\n",
    "            \n",
    "            for user1 in user_chunk:\n",
    "                times1 = user_post_times[user1]\n",
    "                # Only compare with a subset of users to reduce computation\n",
    "                comparison_users = random.sample(users, min(200, len(users)))\n",
    "                \n",
    "                for user2 in comparison_users:\n",
    "                    if user1 == user2:\n",
    "                        continue\n",
    "                        \n",
    "                    times2 = user_post_times[user2]\n",
    "                    \n",
    "                    # Sample times for efficiency\n",
    "                    max_samples = 10\n",
    "                    if len(times1) > max_samples:\n",
    "                        times1_sample = random.sample(times1, max_samples)\n",
    "                    else:\n",
    "                        times1_sample = times1\n",
    "                        \n",
    "                    if len(times2) > max_samples:\n",
    "                        times2_sample = random.sample(times2, max_samples)\n",
    "                    else:\n",
    "                        times2_sample = times2\n",
    "                    \n",
    "                    # Count time-based interactions\n",
    "                    interactions = 0\n",
    "                    for t1 in times1_sample:\n",
    "                        for t2 in times2_sample:\n",
    "                            if abs(t1 - t2) < temporal_window:\n",
    "                                interactions += 1\n",
    "                    \n",
    "                    # Normalize and add edge if interactions found\n",
    "                    if interactions > 0:\n",
    "                        weight = interactions / (len(times1_sample) * len(times2_sample))\n",
    "                        edges.append((\n",
    "                            self.user_to_node[user1],\n",
    "                            self.user_to_node[user2],\n",
    "                            weight\n",
    "                        ))\n",
    "        \n",
    "        # Add edges to graph\n",
    "        for u, v, w in edges:\n",
    "            G.add_edge(u, v, weight=w)\n",
    "            \n",
    "        self.graph = G\n",
    "        self.log(f\"Graph constructed with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")\n",
    "        \n",
    "        # Create adjacency matrix for the graph encoder (using updated API)\n",
    "        adj = nx.adjacency_matrix(G, weight='weight')\n",
    "        adj = adj.tocoo()\n",
    "        \n",
    "        # Normalize adjacency matrix\n",
    "        degrees = np.array(adj.sum(axis=1)).flatten()\n",
    "        degree_mat_inv_sqrt = 1.0 / np.sqrt(np.maximum(degrees, 1e-12))\n",
    "        \n",
    "        # Create dense adjacency matrix\n",
    "        rows, cols = adj.row, adj.col\n",
    "        adj_data = adj.data * degree_mat_inv_sqrt[rows] * degree_mat_inv_sqrt[cols]\n",
    "        \n",
    "        # Convert to dense tensor\n",
    "        adj_dense = torch.zeros((G.number_of_nodes(), G.number_of_nodes()), device=device)\n",
    "        for i in range(len(rows)):\n",
    "            adj_dense[rows[i], cols[i]] = adj_data[i]\n",
    "        \n",
    "        self.adj_matrix = adj_dense\n",
    "        \n",
    "        return G\n",
    "    \n",
    "    def train_model(self, epochs=30, learning_rate=0.001):\n",
    "        \"\"\"Train the feature and graph encoders\"\"\"\n",
    "        self.log(\"Training model...\")\n",
    "        \n",
    "        # Initialize models\n",
    "        input_dim = self.feature_matrix.shape[1]\n",
    "        self.feature_encoder = FeatureEncoder(input_dim=input_dim).to(device)\n",
    "        self.graph_encoder = GraphEncoder(\n",
    "            input_dim=32,  # Match the output dimension of feature encoder\n",
    "            hidden_dim=16,\n",
    "            output_dim=8\n",
    "        ).to(device)\n",
    "        \n",
    "        # Convert feature matrix to tensor\n",
    "        X = torch.FloatTensor(self.feature_matrix).to(device)\n",
    "        \n",
    "        # Optimizer\n",
    "        optimizer = Adam(list(self.feature_encoder.parameters()) + \n",
    "                         list(self.graph_encoder.parameters()),\n",
    "                         lr=learning_rate)\n",
    "        \n",
    "        # Training loop\n",
    "        self.feature_encoder.train()\n",
    "        self.graph_encoder.train()\n",
    "        \n",
    "        losses = []\n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass through feature encoder\n",
    "            feature_embeddings = self.feature_encoder(X)\n",
    "            \n",
    "            # Forward pass through graph encoder\n",
    "            graph_embeddings = self.graph_encoder(feature_embeddings, self.adj_matrix)\n",
    "            \n",
    "            # Self-supervised contrastive loss\n",
    "            src, dst = torch.nonzero(self.adj_matrix > 0, as_tuple=True)\n",
    "            \n",
    "            if len(src) > 0:\n",
    "                # Positive pairs (users who post at similar times)\n",
    "                pos_scores = torch.sum(graph_embeddings[src] * graph_embeddings[dst], dim=1)\n",
    "                pos_loss = -torch.mean(F.logsigmoid(pos_scores))\n",
    "                \n",
    "                # Negative sampling (random user pairs)\n",
    "                neg_samples = 5\n",
    "                neg_src = torch.randint(0, X.size(0), (len(src) * neg_samples,), device=device)\n",
    "                neg_dst = torch.randint(0, X.size(0), (len(src) * neg_samples,), device=device)\n",
    "                neg_scores = torch.sum(graph_embeddings[neg_src] * graph_embeddings[neg_dst], dim=1)\n",
    "                neg_loss = -torch.mean(F.logsigmoid(-neg_scores))\n",
    "                \n",
    "                graph_loss = pos_loss + neg_loss\n",
    "            else:\n",
    "                graph_loss = torch.tensor(0.0, device=device)\n",
    "            \n",
    "            # Feature reconstruction loss\n",
    "            reconstruction_loss = F.mse_loss(feature_embeddings, feature_embeddings.detach())\n",
    "            \n",
    "            # Combined loss\n",
    "            loss = reconstruction_loss + 0.5 * graph_loss\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "                self.log(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.6f}\")\n",
    "        \n",
    "        self.log(\"Training completed\")\n",
    "        return losses\n",
    "    \n",
    "    def generate_embeddings(self):\n",
    "        \"\"\"Generate user embeddings\"\"\"\n",
    "        self.log(\"Generating embeddings...\")\n",
    "        \n",
    "        # Set models to evaluation mode\n",
    "        self.feature_encoder.eval()\n",
    "        self.graph_encoder.eval()\n",
    "        \n",
    "        # Generate embeddings\n",
    "        with torch.no_grad():\n",
    "            X = torch.FloatTensor(self.feature_matrix).to(device)\n",
    "            feature_embeddings = self.feature_encoder(X)\n",
    "            final_embeddings = self.graph_encoder(feature_embeddings, self.adj_matrix)\n",
    "            embeddings = final_embeddings.cpu().numpy()\n",
    "        \n",
    "        # Store user embeddings\n",
    "        user_embeddings = {}\n",
    "        for node_id, embedding in enumerate(embeddings):\n",
    "            user = self.node_to_user.get(node_id)\n",
    "            if user:\n",
    "                user_embeddings[user] = embedding\n",
    "        \n",
    "        self.user_embeddings = user_embeddings\n",
    "        return user_embeddings\n",
    "    \n",
    "    def detect_bots(self, num_clusters=2):\n",
    "        \"\"\"Detect bots using clustering\"\"\"\n",
    "        self.log(\"Detecting bots...\")\n",
    "        \n",
    "        if not self.user_embeddings:\n",
    "            self.log(\"No embeddings found. Generating embeddings...\")\n",
    "            self.generate_embeddings()\n",
    "        \n",
    "        # Prepare data for clustering\n",
    "        users = list(self.user_embeddings.keys())\n",
    "        embeddings = np.array([self.user_embeddings[user] for user in users])\n",
    "        \n",
    "        # Apply KMeans clustering\n",
    "        kmeans = KMeans(n_clusters=num_clusters, random_state=SEED, n_init=10)\n",
    "        labels = kmeans.fit_predict(embeddings)\n",
    "        \n",
    "        # Determine which cluster is bots\n",
    "        # Bots typically form a more cohesive cluster\n",
    "        cluster_sizes = {label: np.sum(labels == label) for label in range(num_clusters)}\n",
    "        \n",
    "        # Calculate cluster cohesion\n",
    "        cluster_cohesion = {}\n",
    "        for label in range(num_clusters):\n",
    "            cluster_points = embeddings[labels == label]\n",
    "            if len(cluster_points) > 1:\n",
    "                # Use pairwise distances within cluster\n",
    "                distances = []\n",
    "                for i in range(min(len(cluster_points), 100)):  # Sample for efficiency\n",
    "                    idx = np.random.choice(len(cluster_points))\n",
    "                    point = cluster_points[idx]\n",
    "                    dists = np.linalg.norm(cluster_points - point, axis=1)\n",
    "                    distances.extend(dists.tolist())\n",
    "                cluster_cohesion[label] = np.mean(distances)\n",
    "            else:\n",
    "                cluster_cohesion[label] = float('inf')\n",
    "        \n",
    "        # Bot cluster is typically smaller and more cohesive\n",
    "        bot_scores = {}\n",
    "        for label in range(num_clusters):\n",
    "            # Normalized size (smaller = higher score)\n",
    "            size_score = 1 - (cluster_sizes[label] / sum(cluster_sizes.values()))\n",
    "            \n",
    "            # Normalized cohesion (more cohesive = higher score)\n",
    "            max_cohesion = max(cluster_cohesion.values())\n",
    "            cohesion_score = 1 - (cluster_cohesion[label] / max_cohesion) if max_cohesion > 0 else 0\n",
    "            \n",
    "            # Combined score (higher = more likely to be bots)\n",
    "            bot_scores[label] = 0.7 * size_score + 0.3 * cohesion_score\n",
    "        \n",
    "        # Bot cluster has highest score\n",
    "        bot_cluster = max(bot_scores.items(), key=lambda x: x[1])[0]\n",
    "        \n",
    "        # Map users to bot probabilities\n",
    "        user_bot_probs = {}\n",
    "        for i, user in enumerate(users):\n",
    "            # Calculate distance to centroids\n",
    "            user_embedding = embeddings[i]\n",
    "            distances = []\n",
    "            \n",
    "            for label in range(num_clusters):\n",
    "                centroid = kmeans.cluster_centers_[label]\n",
    "                dist = np.linalg.norm(user_embedding - centroid)\n",
    "                distances.append(dist)\n",
    "            \n",
    "            # If this user's cluster is the bot cluster\n",
    "            if labels[i] == bot_cluster:\n",
    "                # Calculate probability based on distance to centroids\n",
    "                total_dist = sum(distances)\n",
    "                if total_dist > 0:\n",
    "                    # Invert distance to centroid (closer = higher probability)\n",
    "                    bot_prob = 1 - (distances[bot_cluster] / total_dist)\n",
    "                    bot_prob = min(0.95, max(0.5, bot_prob))  # Clamp between 0.5 and 0.95\n",
    "                else:\n",
    "                    bot_prob = 0.75\n",
    "            else:\n",
    "                # Not in bot cluster\n",
    "                total_dist = sum(distances)\n",
    "                if total_dist > 0:\n",
    "                    # Calculate probability (closer to bot centroid = higher)\n",
    "                    human_prob = 1 - (distances[labels[i]] / total_dist)\n",
    "                    bot_prob = 1 - human_prob\n",
    "                    bot_prob = min(0.49, max(0.05, bot_prob))  # Clamp between 0.05 and 0.49\n",
    "                else:\n",
    "                    bot_prob = 0.25\n",
    "            \n",
    "            user_bot_probs[user] = bot_prob\n",
    "        \n",
    "        self.bot_scores = user_bot_probs\n",
    "        \n",
    "        # Log results\n",
    "        bot_users = [user for user, prob in user_bot_probs.items() if prob >= 0.5]\n",
    "        total_users = len(user_bot_probs)\n",
    "        self.log(f\"Detected {len(bot_users)} bots out of {total_users} users ({len(bot_users)/total_users*100:.1f}%)\")\n",
    "        \n",
    "        return user_bot_probs\n",
    "    \n",
    "    def visualize_results(self, output_file=None):\n",
    "        \"\"\"Visualize bot detection results\"\"\"\n",
    "        self.log(\"Visualizing results...\")\n",
    "        \n",
    "        if not self.user_embeddings or not self.bot_scores:\n",
    "            self.log(\"No results to visualize\")\n",
    "            return\n",
    "        \n",
    "        # Prepare data\n",
    "        users = list(self.bot_scores.keys())\n",
    "        embeddings = np.array([self.user_embeddings[user] for user in users])\n",
    "        is_bot = np.array([self.bot_scores[user] >= 0.5 for user in users])\n",
    "        \n",
    "        # Use PCA for visualization\n",
    "        pca = PCA(n_components=2)\n",
    "        embeddings_2d = pca.fit_transform(embeddings)\n",
    "        \n",
    "        # Create plot\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        \n",
    "        # Plot human users\n",
    "        plt.scatter(\n",
    "            embeddings_2d[~is_bot, 0],\n",
    "            embeddings_2d[~is_bot, 1],\n",
    "            c='blue',\n",
    "            alpha=0.7,\n",
    "            label='Human'\n",
    "        )\n",
    "        \n",
    "        # Plot bot users\n",
    "        plt.scatter(\n",
    "            embeddings_2d[is_bot, 0],\n",
    "            embeddings_2d[is_bot, 1],\n",
    "            c='red',\n",
    "            alpha=0.7,\n",
    "            label='Bot'\n",
    "        )\n",
    "        \n",
    "        plt.title('Bot Detection Results', fontsize=16)\n",
    "        plt.legend(fontsize=12)\n",
    "        plt.grid(alpha=0.3)\n",
    "        \n",
    "        if output_file is None:\n",
    "            output_file = os.path.join(self.output_dir, \"bot_detection_results.png\")\n",
    "            \n",
    "        plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "        self.log(f\"Visualization saved to {output_file}\")\n",
    "    \n",
    "    def analyze_feature_importance(self):\n",
    "        \"\"\"Analyze which features are most important for bot detection\"\"\"\n",
    "        self.log(\"Analyzing feature importance...\")\n",
    "        \n",
    "        if not hasattr(self, 'user_features') or self.user_features is None or self.bot_scores is None:\n",
    "            self.log(\"Feature importance analysis requires user features and bot scores\")\n",
    "            return None\n",
    "        \n",
    "        # Add bot labels to user features\n",
    "        analysis_df = self.user_features.copy()\n",
    "        analysis_df['is_bot'] = analysis_df['user'].map(\n",
    "            {user: 1 if prob >= 0.5 else 0 for user, prob in self.bot_scores.items()}\n",
    "        )\n",
    "        \n",
    "        # Calculate mean feature values for bots vs. humans\n",
    "        feature_cols = [col for col in analysis_df.columns if col not in ['user', 'is_bot']]\n",
    "        \n",
    "        bot_means = analysis_df[analysis_df['is_bot'] == 1][feature_cols].mean()\n",
    "        human_means = analysis_df[analysis_df['is_bot'] == 0][feature_cols].mean()\n",
    "        \n",
    "        # Calculate difference\n",
    "        diff = bot_means - human_means\n",
    "        abs_diff = diff.abs()\n",
    "        \n",
    "        # Sort by importance\n",
    "        importance = abs_diff.sort_values(ascending=False)\n",
    "        \n",
    "        # Visualize top 10 features\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        importance[:10].plot(kind='bar')\n",
    "        plt.title('Top 10 Discriminative Features', fontsize=14)\n",
    "        plt.ylabel('Absolute Difference (Bot - Human)', fontsize=12)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Plot comparison of top 5 features\n",
    "        plt.subplot(1, 2, 2)\n",
    "        top_features = importance.index[:5]\n",
    "        \n",
    "        bot_vals = bot_means[top_features].values\n",
    "        human_vals = human_means[top_features].values\n",
    "        \n",
    "        x = np.arange(len(top_features))\n",
    "        width = 0.35\n",
    "        \n",
    "        plt.bar(x - width/2, bot_vals, width, label='Bot')\n",
    "        plt.bar(x + width/2, human_vals, width, label='Human')\n",
    "        \n",
    "        plt.title('Bot vs Human Feature Comparison', fontsize=14)\n",
    "        plt.xticks(x, top_features, rotation=45, ha='right')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save plot\n",
    "        importance_plot_path = os.path.join(self.output_dir, \"feature_importance.png\")\n",
    "        plt.savefig(importance_plot_path, dpi=300, bbox_inches='tight')\n",
    "        self.log(f\"Feature importance visualization saved to {importance_plot_path}\")\n",
    "        \n",
    "        # Save detailed analysis to CSV\n",
    "        feature_comparison = pd.DataFrame({\n",
    "            'feature': feature_cols,\n",
    "            'bot_mean': bot_means,\n",
    "            'human_mean': human_means,\n",
    "            'difference': diff,\n",
    "            'abs_difference': abs_diff\n",
    "        }).sort_values('abs_difference', ascending=False)\n",
    "        \n",
    "        importance_csv_path = os.path.join(self.output_dir, \"feature_importance.csv\")\n",
    "        feature_comparison.to_csv(importance_csv_path, index=False)\n",
    "        \n",
    "        return importance\n",
    "    \n",
    "    def compare_with_baselines(self, df):\n",
    "        \"\"\"Compare with baseline methods from literature\"\"\"\n",
    "        self.log(\"Comparing with baseline methods...\")\n",
    "        \n",
    "        if not self.bot_scores:\n",
    "            self.log(\"No bot detection results to compare with baselines\")\n",
    "            return None\n",
    "        \n",
    "        # 1. Simple activity-based heuristic (high posting frequency)\n",
    "        user_post_counts = df['author_handle'].value_counts()\n",
    "        activity_threshold = np.percentile(user_post_counts.values, 95)  # Top 5% most active\n",
    "        activity_bots = set(user_post_counts[user_post_counts > activity_threshold].index)\n",
    "        \n",
    "        # 2. Temporal regularity (consistent posting patterns)\n",
    "        temporal_bots = set()\n",
    "        for user in tqdm(self.unique_users, desc=\"Analyzing temporal patterns\", disable=not self.verbose):\n",
    "            user_posts = df[df['author_handle'] == user]\n",
    "            if len(user_posts) < 5:\n",
    "                continue\n",
    "                \n",
    "            post_times = pd.to_datetime(user_posts['created_at'])\n",
    "            sorted_times = sorted(post_times)\n",
    "            time_diffs = [(sorted_times[i] - sorted_times[i-1]).total_seconds() / 60 \n",
    "                         for i in range(1, len(sorted_times))]\n",
    "            \n",
    "            if len(time_diffs) > 1:\n",
    "                # Calculate coefficient of variation (lower = more regular)\n",
    "                cv = np.std(time_diffs) / np.mean(time_diffs) if np.mean(time_diffs) > 0 else float('inf')\n",
    "                if cv < 0.5:  # Very regular posting pattern\n",
    "                    temporal_bots.add(user)\n",
    "        \n",
    "        # 3. Sentiment-based (low variance in sentiment)\n",
    "        sentiment_bots = set()\n",
    "        for user in tqdm(self.unique_users, desc=\"Analyzing sentiment patterns\", disable=not self.verbose):\n",
    "            user_posts = df[df['author_handle'] == user]\n",
    "            if len(user_posts) < 5:\n",
    "                continue\n",
    "                \n",
    "            # Calculate sentiment entropy\n",
    "            happy_count = np.sum(user_posts['happy'] > 0)\n",
    "            sad_count = np.sum(user_posts['sad'] > 0)\n",
    "            neutral_count = np.sum(user_posts['neutral'] > 0)\n",
    "            \n",
    "            total = happy_count + sad_count + neutral_count\n",
    "            if total > 0:\n",
    "                probs = np.array([happy_count, sad_count, neutral_count]) / total\n",
    "                # Calculate entropy (higher = more diverse sentiment)\n",
    "                entropy = -np.sum(probs * np.log2(probs + 1e-10))\n",
    "                \n",
    "                # Low entropy = uniform sentiment = suspicious\n",
    "                if entropy < 0.8:  # Threshold for low sentiment diversity\n",
    "                    sentiment_bots.add(user)\n",
    "        \n",
    "        # 4. Our method (GMAE2-CGNN)\n",
    "        our_bots = set(user for user, prob in self.bot_scores.items() if prob >= 0.5)\n",
    "        \n",
    "        # Compare methods\n",
    "        self.log(f\"Activity-based: {len(activity_bots)} bots\")\n",
    "        self.log(f\"Temporal regularity: {len(temporal_bots)} bots\")\n",
    "        self.log(f\"Sentiment uniformity: {len(sentiment_bots)} bots\")\n",
    "        self.log(f\"Our GMAE2-CGNN: {len(our_bots)} bots\")\n",
    "        \n",
    "        # Calculate overlap between methods\n",
    "        overlap_activity = len(our_bots.intersection(activity_bots))\n",
    "        overlap_temporal = len(our_bots.intersection(temporal_bots))\n",
    "        overlap_sentiment = len(our_bots.intersection(sentiment_bots))\n",
    "        \n",
    "        self.log(f\"Overlap with activity-based: {overlap_activity} users ({overlap_activity/len(our_bots)*100:.1f}%)\")\n",
    "        self.log(f\"Overlap with temporal: {overlap_temporal} users ({overlap_temporal/len(our_bots)*100:.1f}%)\")\n",
    "        self.log(f\"Overlap with sentiment: {overlap_sentiment} users ({overlap_sentiment/len(our_bots)*100:.1f}%)\")\n",
    "        \n",
    "        # Create comparison visualization\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        \n",
    "        # Venn diagram of bot detections (simplified using sets)\n",
    "        from matplotlib_venn import venn3\n",
    "        \n",
    "        venn = venn3([activity_bots, temporal_bots, our_bots], \n",
    "               ('Activity', 'Temporal', 'GMAE2-CGNN'))\n",
    "        plt.title('Bot Detection Method Comparison', fontsize=16)\n",
    "        \n",
    "        # Save comparison\n",
    "        comparison_path = os.path.join(self.output_dir, \"method_comparison.png\")\n",
    "        plt.savefig(comparison_path, dpi=300, bbox_inches='tight')\n",
    "        self.log(f\"Method comparison visualization saved to {comparison_path}\")\n",
    "        \n",
    "        # Save detailed comparison to CSV\n",
    "        all_users = set(self.user_to_node.keys())\n",
    "        comparison_data = []\n",
    "        \n",
    "        for user in all_users:\n",
    "            if user in self.bot_scores:\n",
    "                comparison_data.append({\n",
    "                    'user': user,\n",
    "                    'activity_bot': 1 if user in activity_bots else 0,\n",
    "                    'temporal_bot': 1 if user in temporal_bots else 0,\n",
    "                    'sentiment_bot': 1 if user in sentiment_bots else 0,\n",
    "                    'gmae2_cgnn_bot': 1 if user in our_bots else 0,\n",
    "                    'gmae2_cgnn_score': self.bot_scores.get(user, 0)\n",
    "                })\n",
    "        \n",
    "        comparison_df = pd.DataFrame(comparison_data)\n",
    "        comparison_csv_path = os.path.join(self.output_dir, \"method_comparison.csv\")\n",
    "        comparison_df.to_csv(comparison_csv_path, index=False)\n",
    "        \n",
    "        return {\n",
    "            'activity_bots': activity_bots,\n",
    "            'temporal_bots': temporal_bots,\n",
    "            'sentiment_bots': sentiment_bots,\n",
    "            'our_bots': our_bots,\n",
    "            'comparison_df': comparison_df\n",
    "        }\n",
    "    \n",
    "    def visualize_bot_network(self, max_nodes=1000):\n",
    "        \"\"\"Visualize the bot subgraph structure\"\"\"\n",
    "        self.log(\"Visualizing bot network structure...\")\n",
    "        \n",
    "        if not self.graph or not self.bot_scores:\n",
    "            self.log(\"No graph or bot scores available\")\n",
    "            return\n",
    "        \n",
    "        # Get bot nodes\n",
    "        bot_users = [user for user, prob in self.bot_scores.items() if prob >= 0.5]\n",
    "        bot_nodes = [self.user_to_node[user] for user in bot_users if user in self.user_to_node]\n",
    "        \n",
    "        # Sample if too many\n",
    "        if len(bot_nodes) > max_nodes:\n",
    "            self.log(f\"Sampling {max_nodes} nodes from {len(bot_nodes)} bot nodes for visualization\")\n",
    "            bot_nodes = random.sample(bot_nodes, max_nodes)\n",
    "        \n",
    "        # Extract bot subgraph\n",
    "        bot_subgraph = self.graph.subgraph(bot_nodes)\n",
    "        \n",
    "        # Visualize\n",
    "        plt.figure(figsize=(14, 12))\n",
    "        \n",
    "        # Get communities in the bot subgraph\n",
    "        try:\n",
    "            communities = nx.community.greedy_modularity_communities(bot_subgraph)\n",
    "            self.log(f\"Detected {len(communities)} communities in the bot network\")\n",
    "            \n",
    "            # Assign colors to communities\n",
    "            colors = plt.cm.rainbow(np.linspace(0, 1, len(communities)))\n",
    "            node_colors = {}\n",
    "            \n",
    "            for i, community in enumerate(communities):\n",
    "                for node in community:\n",
    "                    node_colors[node] = colors[i]\n",
    "            \n",
    "            # Draw with community colors\n",
    "            pos = nx.spring_layout(bot_subgraph, seed=42)\n",
    "            \n",
    "            # Draw nodes colored by community\n",
    "            for node in bot_subgraph.nodes():\n",
    "                nx.draw_networkx_nodes(\n",
    "                    bot_subgraph, pos,\n",
    "                    nodelist=[node],\n",
    "                    node_color=[node_colors.get(node, 'gray')],\n",
    "                    node_size=50,\n",
    "                    alpha=0.8\n",
    "                )\n",
    "            \n",
    "            # Draw edges\n",
    "            nx.draw_networkx_edges(\n",
    "                bot_subgraph, pos,\n",
    "                edge_color='gray',\n",
    "                width=0.5,\n",
    "                alpha=0.5\n",
    "            )\n",
    "            \n",
    "            plt.title(f'Bot Network Structure: {len(bot_nodes)} bots, {len(communities)} communities', fontsize=16)\n",
    "            plt.axis('off')\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.log(f\"Error detecting communities: {e}\")\n",
    "            \n",
    "            # Fallback to basic visualization\n",
    "            pos = nx.spring_layout(bot_subgraph, seed=42)\n",
    "            nx.draw(bot_subgraph, pos, \n",
    "                    node_color='red',\n",
    "                    node_size=30,\n",
    "                    alpha=0.8,\n",
    "                    edge_color='gray',\n",
    "                    width=0.5)\n",
    "            plt.title(f'Bot Network Structure (Sample of {len(bot_nodes)} bots)', fontsize=16)\n",
    "            plt.axis('off')\n",
    "        \n",
    "        # Save visualization\n",
    "        network_vis_path = os.path.join(self.output_dir, \"bot_network.png\")\n",
    "        plt.savefig(network_vis_path, dpi=300, bbox_inches='tight')\n",
    "        self.log(f\"Bot network visualization saved to {network_vis_path}\")\n",
    "    \n",
    "    def save_model(self, path=None):\n",
    "        \"\"\"Save the trained model for later use\"\"\"\n",
    "        if path is None:\n",
    "            path = os.path.join(self.output_dir, \"gmae2_cgnn_model.pt\")\n",
    "            \n",
    "        try:\n",
    "            if not self.feature_encoder or not self.graph_encoder:\n",
    "                self.log(\"No trained model to save\")\n",
    "                return False\n",
    "                \n",
    "            model_dict = {\n",
    "                'feature_encoder': self.feature_encoder.state_dict(),\n",
    "                'graph_encoder': self.graph_encoder.state_dict(),\n",
    "                'user_to_node': self.user_to_node,\n",
    "                'node_to_user': self.node_to_user,\n",
    "                'bot_scores': self.bot_scores,\n",
    "            }\n",
    "            \n",
    "            torch.save(model_dict, path)\n",
    "            self.log(f\"Model saved to {path}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            self.log(f\"Error saving model: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def load_model(self, path):\n",
    "        \"\"\"Load a trained model\"\"\"\n",
    "        try:\n",
    "            if not os.path.exists(path):\n",
    "                self.log(f\"Model file {path} not found\")\n",
    "                return False\n",
    "            \n",
    "            model_dict = torch.load(path, map_location=device)\n",
    "            \n",
    "            # Load user mappings\n",
    "            self.user_to_node = model_dict['user_to_node']\n",
    "            self.node_to_user = model_dict['node_to_user']\n",
    "            self.bot_scores = model_dict['bot_scores']\n",
    "            \n",
    "            # Recreate models and load weights\n",
    "            if 'feature_encoder' in model_dict:\n",
    "                # Infer input dim from the first layer\n",
    "                input_weight = next(iter(model_dict['feature_encoder'].items()))[1]\n",
    "                input_dim = input_weight.shape[1]\n",
    "                \n",
    "                self.feature_encoder = FeatureEncoder(input_dim=input_dim).to(device)\n",
    "                self.feature_encoder.load_state_dict(model_dict['feature_encoder'])\n",
    "            \n",
    "            if 'graph_encoder' in model_dict:\n",
    "                # Get dimensions from the weights\n",
    "                first_layer = list(model_dict['graph_encoder'].items())[0][1]\n",
    "                input_dim = first_layer.shape[1]\n",
    "                \n",
    "                self.graph_encoder = GraphEncoder(input_dim=input_dim).to(device)\n",
    "                self.graph_encoder.load_state_dict(model_dict['graph_encoder'])\n",
    "            \n",
    "            self.log(f\"Model loaded from {path}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            self.log(f\"Error loading model: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def run_pipeline(self, df, sample_size=None):\n",
    "        \"\"\"Run the complete bot detection pipeline\"\"\"\n",
    "        self.log(\"Running bot detection pipeline...\")\n",
    "        self.run_stats['start_time'] = datetime.now()\n",
    "        \n",
    "        # Sample data if needed\n",
    "        if sample_size and len(df) > sample_size:\n",
    "            self.log(f\"Sampling {sample_size} rows from {len(df)} total rows\")\n",
    "            df_sample = df.sample(n=sample_size, random_state=SEED)\n",
    "        else:\n",
    "            df_sample = df\n",
    "        \n",
    "        # Process data\n",
    "        processed_df = self.preprocess_data(df_sample)\n",
    "        self.extract_features(processed_df)\n",
    "        self.construct_graph(processed_df)\n",
    "        self.train_model()\n",
    "        self.generate_embeddings()\n",
    "        bot_probs = self.detect_bots()\n",
    "        \n",
    "        # Generate research outputs\n",
    "        self.visualize_results()\n",
    "        self.analyze_feature_importance()\n",
    "        self.compare_with_baselines(processed_df)\n",
    "        self.visualize_bot_network()\n",
    "        \n",
    "        # Save model and results\n",
    "        self.save_model()\n",
    "        \n",
    "        # Save results to CSV\n",
    "        results_df = pd.DataFrame({\n",
    "            'user': list(bot_probs.keys()),\n",
    "            'bot_probability': list(bot_probs.values()),\n",
    "            'is_bot': [1 if p >= 0.5 else 0 for p in bot_probs.values()]\n",
    "        })\n",
    "        \n",
    "        csv_path = os.path.join(self.output_dir, \"bot_detection_results.csv\")\n",
    "        results_df.to_csv(csv_path, index=False)\n",
    "        self.log(f\"Results saved to {csv_path}\")\n",
    "        \n",
    "        # Update statistics\n",
    "        num_bots = sum(1 for p in bot_probs.values() if p >= 0.5)\n",
    "        total_users = len(bot_probs)\n",
    "        bot_percentage = (num_bots / total_users) * 100 if total_users > 0 else 0\n",
    "        \n",
    "        self.run_stats['end_time'] = datetime.now()\n",
    "        self.run_stats['total_users'] = total_users\n",
    "        self.run_stats['detected_bots'] = num_bots\n",
    "        self.run_stats['bot_percentage'] = bot_percentage\n",
    "        \n",
    "        # Save run statistics\n",
    "        stats_path = os.path.join(self.output_dir, \"run_statistics.json\")\n",
    "        with open(stats_path, 'w') as f:\n",
    "            import json\n",
    "            # Convert datetime to string\n",
    "            stats_dict = self.run_stats.copy()\n",
    "            stats_dict['start_time'] = str(stats_dict['start_time'])\n",
    "            stats_dict['end_time'] = str(stats_dict['end_time'])\n",
    "            json.dump(stats_dict, f, indent=2)\n",
    "        \n",
    "        return bot_probs\n",
    "\n",
    "    def run_full_dataset_pipeline(self, df):\n",
    "        \"\"\"Run the bot detection pipeline on the full dataset using batching with proper deduplication\"\"\"\n",
    "        self.log(\"Running bot detection pipeline on full dataset...\")\n",
    "        self.run_stats['start_time'] = datetime.now()\n",
    "        \n",
    "        # First, preprocess all data\n",
    "        self.log(\"Preprocessing full dataset...\")\n",
    "        # Ensure datetime format for created_at\n",
    "        if not pd.api.types.is_datetime64_dtype(df['created_at']):\n",
    "            df['created_at'] = pd.to_datetime(df['created_at'], format='mixed', utc=True, errors='coerce')\n",
    "        \n",
    "        # Add neutral column if not present\n",
    "        if 'neutral' not in df.columns:\n",
    "            df['neutral'] = ((df['happy'] == 0) & (df['sad'] == 0)).astype(int)\n",
    "        \n",
    "        # Get all users with at least min_posts\n",
    "        min_posts = 2  # The minimum post threshold from original code\n",
    "        user_counts = df['author_handle'].value_counts()\n",
    "        qualified_users = user_counts[user_counts >= min_posts].index.tolist()\n",
    "        total_qualified_users = len(qualified_users)\n",
    "        self.log(f\"Total qualified users with at least {min_posts} posts: {total_qualified_users}\")\n",
    "        \n",
    "        # Track processed users to avoid duplicates\n",
    "        processed_users = set()\n",
    "        all_bot_probs = {}\n",
    "        \n",
    "        # Process in batches\n",
    "        batch_size = 5000  # As used in the original code\n",
    "        num_batches = (total_qualified_users + batch_size - 1) // batch_size\n",
    "        \n",
    "        for batch_idx in range(num_batches):\n",
    "            self.log(f\"Processing batch {batch_idx+1}/{num_batches}\")\n",
    "            \n",
    "            # Get users for this batch that haven't been processed yet\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = min((batch_idx + 1) * batch_size, total_qualified_users)\n",
    "            batch_candidates = qualified_users[start_idx:end_idx]\n",
    "            \n",
    "            # Filter out already processed users\n",
    "            batch_users = [user for user in batch_candidates if user not in processed_users]\n",
    "            \n",
    "            if not batch_users:\n",
    "                self.log(f\"Skipping batch {batch_idx+1} - all users already processed\")\n",
    "                continue\n",
    "                \n",
    "            self.log(f\"Processing {len(batch_users)} new users in batch {batch_idx+1}\")\n",
    "            \n",
    "            # Extract data for these users\n",
    "            batch_df = df[df['author_handle'].isin(batch_users)]\n",
    "            \n",
    "            # Reset state for new batch\n",
    "            self.user_to_node = {}\n",
    "            self.node_to_user = {}\n",
    "            self.graph = None\n",
    "            self.adj_matrix = None\n",
    "            self.user_embeddings = None\n",
    "            self.bot_scores = None\n",
    "            \n",
    "            # Run the pipeline on this batch\n",
    "            try:\n",
    "                self.preprocess_data(batch_df, max_users=len(batch_users), min_posts=min_posts)\n",
    "                self.extract_features(batch_df)\n",
    "                self.construct_graph(batch_df)\n",
    "                \n",
    "                # Skip if graph couldn't be constructed properly\n",
    "                if self.graph is None or self.graph.number_of_nodes() < 2:\n",
    "                    self.log(f\"Skipping batch {batch_idx+1} due to insufficient graph structure\")\n",
    "                    continue\n",
    "                    \n",
    "                self.train_model()\n",
    "                self.generate_embeddings()\n",
    "                batch_bot_probs = self.detect_bots()\n",
    "                \n",
    "                # Mark these users as processed\n",
    "                batch_processed_users = set(batch_bot_probs.keys())\n",
    "                processed_users.update(batch_processed_users)\n",
    "                \n",
    "                # Sanity check - detect any potential duplicates\n",
    "                duplicate_count = len(set(all_bot_probs.keys()) & batch_processed_users)\n",
    "                if duplicate_count > 0:\n",
    "                    self.log(f\"Warning: Found {duplicate_count} duplicate users - these should have been filtered!\")\n",
    "                \n",
    "                # Store results without duplicates\n",
    "                for user, prob in batch_bot_probs.items():\n",
    "                    if user not in all_bot_probs:  # Ensure no duplicates\n",
    "                        all_bot_probs[user] = prob\n",
    "                \n",
    "                # Save interim results for this batch\n",
    "                interim_df = pd.DataFrame({\n",
    "                    'user': list(batch_bot_probs.keys()),\n",
    "                    'bot_probability': list(batch_bot_probs.values()),\n",
    "                    'is_bot': [1 if p >= 0.5 else 0 for p in batch_bot_probs.values()]\n",
    "                })\n",
    "                \n",
    "                interim_path = os.path.join(self.output_dir, f\"batch_{batch_idx+1}_results.csv\")\n",
    "                interim_df.to_csv(interim_path, index=False)\n",
    "                \n",
    "                # Update batch statistics\n",
    "                num_batch_bots = sum(1 for p in batch_bot_probs.values() if p >= 0.5)\n",
    "                batch_stats = {\n",
    "                    'batch': batch_idx + 1,\n",
    "                    'users': len(batch_bot_probs),\n",
    "                    'bots': num_batch_bots,\n",
    "                    'bot_percentage': (num_batch_bots / len(batch_bot_probs)) * 100 if batch_bot_probs else 0\n",
    "                }\n",
    "                self.run_stats['batches'].append(batch_stats)\n",
    "                \n",
    "                # Log accurate running count\n",
    "                self.log(f\"Batch {batch_idx+1}: Processed {len(batch_processed_users)} users, unique total so far: {len(all_bot_probs)}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.log(f\"Error processing batch {batch_idx+1}: {str(e)}\")\n",
    "                import traceback\n",
    "                self.log(traceback.format_exc())\n",
    "                continue\n",
    "            \n",
    "            # Clear memory\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "        \n",
    "        # Combine all results\n",
    "        final_df = pd.DataFrame({\n",
    "            'user': list(all_bot_probs.keys()),\n",
    "            'bot_probability': list(all_bot_probs.values()),\n",
    "            'is_bot': [1 if p >= 0.5 else 0 for p in all_bot_probs.values()]\n",
    "        })\n",
    "        \n",
    "        # Calculate final statistics\n",
    "        bot_count = sum(1 for p in all_bot_probs.values() if p >= 0.5)\n",
    "        bot_percentage = (bot_count / len(all_bot_probs)) * 100 if all_bot_probs else 0\n",
    "        \n",
    "        self.log(f\"Final results: Detected {bot_count} bots out of {len(all_bot_probs)} unique users ({bot_percentage:.1f}%)\")\n",
    "        \n",
    "        # Save final results\n",
    "        final_path = os.path.join(self.output_dir, \"full_dataset_results.csv\")\n",
    "        final_df.to_csv(final_path, index=False)\n",
    "        self.log(f\"Final results saved to {final_path}\")\n",
    "        \n",
    "        # Update statistics\n",
    "        self.run_stats['end_time'] = datetime.now()\n",
    "        self.run_stats['total_users'] = total_qualified_users\n",
    "        self.run_stats['processed_users'] = len(all_bot_probs)\n",
    "        self.run_stats['detected_bots'] = bot_count\n",
    "        self.run_stats['bot_percentage'] = bot_percentage\n",
    "        \n",
    "        # Save run statistics\n",
    "        stats_path = os.path.join(self.output_dir, \"full_run_statistics.json\")\n",
    "        with open(stats_path, 'w') as f:\n",
    "            import json\n",
    "            # Convert datetime to string\n",
    "            stats_dict = self.run_stats.copy()\n",
    "            stats_dict['start_time'] = str(stats_dict['start_time'])\n",
    "            stats_dict['end_time'] = str(stats_dict['end_time'])\n",
    "            json.dump(stats_dict, f, indent=2)\n",
    "        \n",
    "        return all_bot_probs\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the bot detection pipeline on the full dataset\"\"\"\n",
    "    \n",
    "    print(\"Using CPU\")\n",
    "    print(\"Starting GMAE2-CGNN Bot Detection\")\n",
    "    \n",
    "    try:\n",
    "        # Create output directory\n",
    "        output_dir = \"gmae2_cgnn_results\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Load data\n",
    "        print(\"Loading dataset...\")\n",
    "        df1 = pd.read_csv(\"Refugee_data_2023_bluesky_sentiment.csv\")\n",
    "        df2 = pd.read_csv(\"Refugee_data_2024_bluesky_sentiment.csv\")\n",
    "        df = pd.concat([df1, df2])\n",
    "        \n",
    "        # Remove unnamed column if exists\n",
    "        if 'Unnamed: 0' in df.columns:\n",
    "            df = df.drop('Unnamed: 0', axis=1)\n",
    "        \n",
    "        # Load keywords\n",
    "        keywords_df = pd.read_csv(\"refugee_immigrant_keyword_list_v3.csv\")\n",
    "        keywords_df = keywords_df.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "        \n",
    "        # Merge data\n",
    "        merged_df = df.merge(keywords_df, left_on=\"keyword\", right_on=\"term\", how=\"left\")\n",
    "        \n",
    "        # Convert to datetime and add neutral column\n",
    "        merged_df['created_at'] = pd.to_datetime(merged_df['created_at'], format='mixed', utc=True)\n",
    "        merged_df = merged_df[merged_df['created_at'] < \"2025-01-01 18:18:04.312000+0000\"]\n",
    "        merged_df['neutral'] = ((merged_df['happy'] == 0) & (merged_df['sad'] == 0)).astype(int)\n",
    "        \n",
    "        print(f\"Loaded dataset with {len(merged_df)} posts and {len(merged_df['author_handle'].unique())} unique users\")\n",
    "        \n",
    "        # Initialize detector\n",
    "        detector = BotDetector(verbose=True, output_dir=output_dir)\n",
    "        \n",
    "        # Choose between running on a sample or the full dataset\n",
    "        sample_mode = False  # Set to False to run on the full dataset\n",
    "        \n",
    "        if sample_mode:\n",
    "            # Run on a sample\n",
    "            sample_size = 100000  # Adjust based on memory constraints\n",
    "            bot_probs = detector.run_pipeline(merged_df, sample_size=sample_size)\n",
    "        else:\n",
    "            # Run on the full dataset using batching\n",
    "            bot_probs = detector.run_full_dataset_pipeline(merged_df)\n",
    "        \n",
    "        # Results\n",
    "        bot_users = sum(1 for p in bot_probs.values() if p >= 0.5)\n",
    "        print(f\"Detected {bot_users} bots out of {len(bot_probs)} users ({bot_users/len(bot_probs)*100:.1f}%)\")\n",
    "        print(f\"Results saved to {output_dir}/\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87c5bfeb-8696-4f71-8e1a-ccfc81ea256d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique users: 304898\n",
      "Users with at least 2 posts: 126091 (41.36%)\n",
      "Users with only 1 post: 178807 (58.64%)\n",
      "Difference between total and processed users: 178807\n",
      "\n",
      "Users with at least 2 posts before date filtering: 126091\n",
      "Users with at least 2 posts after date filtering: 126091\n",
      "Users lost due to date filtering: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "df1 = pd.read_csv(\"Refugee_data_2023_bluesky_sentiment.csv\")\n",
    "df2 = pd.read_csv(\"Refugee_data_2024_bluesky_sentiment.csv\")\n",
    "data_df = pd.concat([df1,df2])\n",
    "data_df = data_df.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "keywords_data_path = \"refugee_immigrant_keyword_list_v3.csv\"\n",
    "\n",
    "keywords_df = pd.read_csv(keywords_data_path)\n",
    "keywords_df = keywords_df.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "# Merge posts data with keyword-subcategory mapping\n",
    "merged_df = data_df.merge(keywords_df, left_on=\"keyword\", right_on=\"term\", how=\"left\")\n",
    "\n",
    "# Convert 'created_at' column to datetime format\n",
    "merged_df['created_at'] = pd.to_datetime(merged_df['created_at'], format='mixed', utc=True)\n",
    "merged_df = merged_df[merged_df['created_at']<\"2025-01-01 18:18:04.312000+0000\"]\n",
    "# Add 'neutral' column: 1 if both happy and sad are 0, else 0\n",
    "merged_df['neutral'] = ((merged_df['happy'] == 0) & (merged_df['sad'] == 0)).astype(int)\n",
    "\n",
    "# Count posts per user\n",
    "user_counts = merged_df['author_handle'].value_counts()\n",
    "\n",
    "# Analyze\n",
    "total_users = len(user_counts)\n",
    "users_with_at_least_2_posts = sum(user_counts >= 2)\n",
    "users_with_only_1_post = sum(user_counts == 1)\n",
    "\n",
    "# Calculate percentages\n",
    "percent_with_at_least_2 = (users_with_at_least_2_posts / total_users) * 100\n",
    "percent_with_only_1 = (users_with_only_1_post / total_users) * 100\n",
    "\n",
    "# Print results\n",
    "print(f\"Total unique users: {total_users}\")\n",
    "print(f\"Users with at least 2 posts: {users_with_at_least_2_posts} ({percent_with_at_least_2:.2f}%)\")\n",
    "print(f\"Users with only 1 post: {users_with_only_1_post} ({percent_with_only_1:.2f}%)\")\n",
    "print(f\"Difference between total and processed users: {total_users - users_with_at_least_2_posts}\")\n",
    "\n",
    "# Check how filtering by date affects user counts\n",
    "if 'created_at' in merged_df.columns:\n",
    "    # Convert to datetime\n",
    "    merged_df['created_at'] = pd.to_datetime(merged_df['created_at'], format='mixed', utc=True, errors='coerce')\n",
    "    \n",
    "    # Count users before date filtering\n",
    "    before_filter_counts = merged_df['author_handle'].value_counts()\n",
    "    users_before_filter = sum(before_filter_counts >= 2)\n",
    "    \n",
    "    # Apply date filter\n",
    "    filtered_df = merged_df[merged_df['created_at'] < \"2025-01-01 18:18:04.312000+0000\"]\n",
    "    \n",
    "    # Count users after date filtering\n",
    "    after_filter_counts = filtered_df['author_handle'].value_counts()\n",
    "    users_after_filter = sum(after_filter_counts >= 2)\n",
    "    \n",
    "    print(f\"\\nUsers with at least 2 posts before date filtering: {users_before_filter}\")\n",
    "    print(f\"Users with at least 2 posts after date filtering: {users_after_filter}\")\n",
    "    print(f\"Users lost due to date filtering: {users_before_filter - users_after_filter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04995f66-27b4-4ede-9f5c-4161c57eb2a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
